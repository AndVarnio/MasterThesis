%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Instructions for using this template:
% This title page is capable of being compiled as is. This is not useful for
% including it in another document. To do this, you have two options:
%
% 1) Copy/paste everything between \begin{document} and \end{document}
% starting at \begin{titlepage} and paste this into another LaTeX file where you
% want your title page.
% OR
% 2) Remove everything outside the \begin{titlepage} and \end{titlepage} and
% move this file to the same directory as the LaTeX file you wish to add it to.
% Then add \input{./title_page_1.tex} to your LaTeX file where you want your
% title page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt, a4paper, usenames, dvipsnames]{report}
\usepackage{fancyhdr} %Header og footer
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{subcaption}
\usepackage{parskip}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{hyperref}
\usepackage{placeins} %FloatBarrier
\usepackage{tabularx}
\usepackage{tikz}						%geometric/algebraic description.
\usepackage[toc,page]{appendix}
\usepackage{pdfpages}
\usetikzlibrary{arrows,shapes,snakes,
		       automata,backgrounds,
		       petri,topaths}				%To use diverse features from tikz

\definecolor{hue}{RGB}{232,238,247} % Lyseblågrå

\usepackage{listings}
\usepackage[skins,listings,breakable]{tcolorbox}
  \usepackage{colortbl}
  \definecolor{Lightgray}{RGB}{235,235,235}


\pagestyle{fancy}
\lhead{\leftmark}
\rhead{\includegraphics[width=4.5cm]{images/logo_ntnu_u-slagord.png}}
\setlength{\headsep}{2cm}
% set the default code style
\lstset{
    basicstyle=\small,
    frame=tb, % draw a frame at the top and bottom of the code block
    tabsize=2, % tab space width
    showstringspaces=false, % don't mark spaces in strings
    numbers=left, % display line numbers on the left
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red}, % string color
    breaklines=true,
}
\newcommand*{\myfont}{\fontfamily{emph}\selectfont}
\begin{document}

% \begin{titlepage}

% \newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

% \center % Center everything on the page

% %----------------------------------------------------------------------------------------
% %	HEADING SECTIONS
% %----------------------------------------------------------------------------------------

% \textsc{\LARGE Norwegian University of Science and Technology}\\[1.5cm] % Name of your university/college

% %	LOGO SECTION
% %----------------------------------------------------------------------------------------

% \includegraphics[width=7.5cm]{images/hypso-logo.png}\\[1cm] % Include a department/university logo - this will require the graphicx package

% %----------------------------------------------------------------------------------------

% \textsc{\Large Master thesis}\\[0.5cm] % Major heading such as course name
% %\textsc{\large Onboard Communication for Hyperspectral Imaging Payload}\\[0.5cm] % Minor heading such as course title

% %----------------------------------------------------------------------------------------
% %	TITLE SECTION
% %----------------------------------------------------------------------------------------

% \HRule \\[0.4cm]
% { \huge \bfseries Assembly and testing of baseline processing chain}\\[0.4cm] % Title of your document
% \HRule \\[1.5cm]

% %----------------------------------------------------------------------------------------
% %	AUTHOR SECTION
% %----------------------------------------------------------------------------------------

% \begin{minipage}{0.4\textwidth}
% \begin{flushleft} \large
% \emph{Author:}\\
% Andreas \textsc{Varntresk} % Your name
% \end{flushleft}
% \end{minipage}
% ~
% \begin{minipage}{0.4\textwidth}
% \begin{flushright} \large
% \emph{Supervisor:} \\
% Kjetil \textsc{Svarstad} \\% Supervisor's Name
% Milica \textsc{Orlandic} % Supervisor's Name
% \end{flushright}
% \end{minipage}\\[2cm]

% % If you don't want a supervisor, uncomment the two lines below and remove the section above
% %\Large \emph{Author:}\\
% %John \textsc{Smith}\\[3cm] % Your name

% %----------------------------------------------------------------------------------------
% %	DATE SECTION
% %----------------------------------------------------------------------------------------

% {\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

% %----------------------------------------------------------------------------------------


% \vfill % Fill the rest of the page with whitespace

% \end{titlepage}

\pagenumbering{gobble}
\begin{abstract}
A design for a baseline image processing chain supposed to be used on a satellite for ocean surveillance is made. The design can capture frames from the camera, compress and store the data. The system consists of an IDS camera and heterogeneous Zynq-7000 SoC with programmable logic and processing system. The images are captured by the processing system, whereas the compression is performed in programmable logic. The compression algorithm used is CCSDS-123. To transfer data between the programmable logic and the processing system a specialized DMA module for streaming hyperspectral images is used. The goal of building a baseline image processing chain has been successfully achieved. However some  additional work must be done for the system to be space-ready.

There is a goal of capturing 2254 12-bit images at a resolution of 1216$\times$1936 with a framerate of 32 FPS, this goal is shown to not be possible with this hardware. However a few reduced example solutions is given. Median binning on the frames captured is also tested.
\end{abstract}

\newpage

\section*{Sammendrag}
Et design for en baseline bildebehandlingskjede som skal brukes på en satellitt for havovervåking, er lagd. Designet kan ta bilder med et kamera, komprimere og lagre dataene. Systemet består av et IDS-kamera og en heterogen Zynq-7000 SoC med programmerbar logikk og prosesseringssystem. Bildene takes av prosesseringssystem, mens kompresjonen utføres i den programmerbare logikken. Komprimeringsalgoritmen som brukes er CCSDS-123. For å overføre data mellom den programmerbare logikken og prosesseringssystem, brukes en spesialisert DMA-modul for streaming av hyperspektrale bilder. Målet med å bygge en baseline bildebehandlingskjeden har blitt oppnådd. Men det må gjøres noe ekstra arbeid for at systemet skal være klar til bruk i værdesnrommet.

Det er et mål om å fange 2254 12-biters bilder med en oppløsning på 1216$\times$1936 med en framerate på 32 FPS, dette målet er vist å ikke være mulig med denne maskinvaren. Men noen reduserte eksempelløsninger er gitt. Median binning på bildene fanget er også testet.

\newpage
\section*{Preface}
This thesis is written at the Department of Electronic systems at NTNU(Norwegian University of Science and Technology) and is the final project of the Electronic Systems Design education program.

I would like to thank my supervisors Milica Orlandic and Kjetil Svarstad for being available to answer all questions i might have had. I would also like to thank the whole HYPSO team for generally being very helpul and easy to work together with.
\newpage
\tableofcontents
\newpage
\listoffigures
\newpage
\listoftables
\newpage
\pagenumbering{arabic}

\chapter{Introduction}

This thesis is written within HYPSO mission framework managed by the NTNU SmallSat, the objective of the thesis is to set up and test a baseline processing chain for hyperspectral imaging on a cubesat. The smallsat lab is a collaboration of graduate students, PhD's and PhD students, working on space and satellite related projects. The goal of the collaboration is to bring together space related activities at NTNU and make it more visible.

The satellite is a cubesat, cubesats are satellites built with the cubesat standard\cite{cubesat}. The size of a cubesat is measured in cubes or units (Us), 1U is a 10 cm cube with a weight of 1-1.33Kg. The satellite in this project is 6U. An image of a 6U cubesat is shown in Figure \ref{fig::smallsatsat} with dimensions 10cm x 20cm x 30cm.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{images/M6P-nanosatellite-bus-1.jpg}
	\caption{6U Satellite\cite{nanoweb}}
	\label{fig::smallsatsat}
\end{figure}

There is essentially two missions going on on-board the satellite, one mission is called SDR-mission(Software Defined Radio) and the other the HSI-mission(Hyper Spectral Imager). The SDR-mission does research on how to better SDR communication with satellites in the arctic region. The HSI-mission also known as the HYPSO(HYPer-spectral Smallsat for ocean Observation) mission is what this project is about, the mission is going to do ocean observations, more specifically gather hyper spectral data from the surface of the ocean with a hyper spectral imager and send the data to earth. The data is supposed to be used to record and predict algae bloom and is ultimately going to be used as early warning system for salmon farms. Too much algae in the water kills salmon, so if a it could be predicted, the fish can be moved before it hits the farms. It is also plans to use the HSI data in combination with UAV's to better do atmospheric corrections of HSI data.

\FloatBarrier
Figure \ref{fig::flyingsatellite} shows concept of operation for the HYPSO mission and the phases of a fly-by for the satellite over the Norwegian sea. The satellite is in sleep mode when it is not in use, then it wakes up, contacts the ground station for receiving of commands, then it captures hyper spectral images of the ocean and connects to a ground station to transfer the data. It may take a few fly-by's before all the data have been transmitted and different ground stations can be used as a receiving station for data. A UAV flies by the same area as the satellite to capture reference images that can be used to better algae detection and atmospheric correction algorithms.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{images/satellitemanouver.png}
	\caption{Satellite fly-by\cite{ntnuinternal}}
	\label{fig::flyingsatellite}
\end{figure}

\FloatBarrier
Figure \ref{fig::satelliteparts} shows how the satellite is supposed to look like inside. The satellite is equipped with a star tracker and a RGB camera pointed in the direction of the earth, used to help knowing the orientation of the satellite, while the reaction wheels and magnetorquers maneuvers the orientation of the satellite. The OPU(On board Processing Unit) processes mission data from the HSI payload, how the different components of the satellite platform is connected is explained later in chapter \ref{chap::satplatform}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/insidesatellite.png}
	\caption{The satellites internal components\cite{anhart}}
	\label{fig::satelliteparts}
\end{figure}

\FloatBarrier
\section{Problem description}
Prior to the master thesis there is a smaller project, called a specialization project. This project was an introduction to the master thesis work. A short summary of the project is included in this thesis, whereas the whole project report can also be found online \cite{specializationproject}. The problem description for the thesis was defined with the knowledge gained from the specialization project and is specified as follows:

{\fontfamily{ptm}\selectfont
  The task is to set up a IDS camera on a Zynq-7000 SoC for capturing of hyperspectral images. With the goal of capturing 2254 12-bit images at a resolution of 1216x1936 with a framerate of 32fps. Aslo investigate how to implement other previous work into the pipeline.

Subtasks for the project in prioritized order:

\begin{enumerate}
    \item Test the new camera and with new parameters for; resolution, number of frames and 12-bit.
    \item Memory management.
    \item Implement and test HW triggering.
    \item Median binning of images.
    \item Possibly investigate CCSDS-123 compression on L1A data.
\end{enumerate}

}
%Want to do processing within 70 seconds
L1A data is unprocessed instrument data at full resolution \cite{sensingterminology}. There is also a constraint that the compressed data must be ready for transfer within 70 seconds after the last frame have been captured.

\section{Contribution}
The contributions of this thesis within the HYPSO missons are mainly two things, the first are  testing in order to explore limitations and rates when integrating camera and hardware, this consists of a few different things listed below.

\begin{itemize}
    \item Implementation of a method for doing median binning with SIMD instructions.
    \item Test of what the maximum possible framerate are for different camera resolutions and bit-depth.
    \item Test of what the rates are possible with median binning compared to mean binning.
    \item Test of what rates are possible with subsampling and binning.
    \item Comparison of HW-triggering and Freerun mode for the camera.
\end{itemize}

The second main contribution is implementation of a basic design for a image processing piepeline of hyperspectral images, consisting of the following tasks: \begin{itemize}
    \item Setup of operating system that can handle DMA operations.
    \item C++ code to implement the SW-modules of the design. In particular, a driver for DMA access and a module for capturing frames.
    \item Kernel module for memory handling.
    \item Integration of HW-implementations for DMA access and CCSDS-123 compression module into the pipeline.
    \item A CSP service with a CSP client application for the design.
\end{itemize}

\section{Structure of the thesis}
This thesis consists of five main chapters. Chapter \ref{chap::specproj} gives a short description of what was done in the specialization project and what was achieved. Chapter \ref{chap::background} explains the theory behind the things that are implemented in the project. Chapter \ref{chap::tests} presents measurements for binning, FPS (Frames Per Second), resolution and different camera modes, and explains how they are achieved. Chapter \ref{chap::pipeline} provides the design for the image processing pipeline and chapter \ref{chap:analDisc} discusses different aspects of the design, the results obtained, and gives some suggestions for future work and then a conclusion is given.

\chapter{Specialization project}
\label{chap::specproj}
In the specialization project the task was to see if it is possible to run a ueye camera on a embedded Linux system on a Zynq-7000 chip. The camera was of type UI-3060CP Rev. 2, it is a USB camera with a Sony IMX174 Cmos monochrome sensor. The goal was to capture 1735 frames at 32 frames per second, where the frames have bit-depth of 12, resolution of 1920x1080 and a binning factor of 20. Then store the data in a cube format. 12-bit frames could not be obtained due to the board only has a USB 2.0 interface, to get a higher bit-depth USB 3.0 must be used.

The camera was tested in software trigger and freerun mode. Timing in software triggering were very unstable so out of these two, freerun mode was by far the best option.
Two different types of binning was tested; median and mean binning, here mean binning was proven to be more susceptive to noise. Different methods of median and mean binning was experimented with. For the mean binning a solution with using SIMD instruction was the fastest, but for median binning none of the tested methods were fast enough on the system. For median binning, many different algorithms were tested, but because of too little time, algorithms with SIMD instruction weren't tested. Three different data cubes was tested, these are BIL, BIP and BSQ, here BIL and BIP cubes was equally fast to create a cube and BSQ the slower one, but when the formats gets stored, BSQ is the faster one and BIL and BIP equally slow.

\chapter{Background}
\label{chap::background}
This chapter explains the tools and theory used in this thesis, the main things are; The theory and basic principles for hyperspectral imaging with a monochrome sensor, this theory is found in the book \emph{Introduction to remote sensing}\cite{remotesensing}. How to use the camera and how to interface with it, this is explained in more detail in the manual for the camera\cite{ueyemanual}. The satellite platform is explained and the system on chip that is part of the HSI payload, the information on these things are found mainly online at ARM, Xilinx and Nano avionics's web pages. The Linux kernel, drivers and memory in Linux is described, the information about this can be found in the books \emph{Operating Systems Internals and Design Principles}\cite{operatingsystems} and \emph{Linux Device Drivers}\cite{ldd3}. A couple FPGA accelerators developed by previous students in the HYPSO project, have been used in this project and is explained in this chapter, three papers have been written on them. The papers and the CCSDS-123 documentation\cite{ccsds123} have been used as a source here, the papers are \emph{An Efficient Real-Time FPGA Implementation of the CCSDS-123 Compression Standard for Hyperspectral Images}\cite{ccsdspaper1}, \emph{A Parallel FPGA Implementation of the CCSDS-123 Compression Algorithm}\cite{ccsdspaper2} and \emph{CubeDMA-Optimizing Three-Dimensional DMA transfers for Hyperspectral Imaging Applications}\cite{cubedmapaper}.

\section{Hyper Spectral Imaging}
Hyperspectral imaging captures a continuous specter of light and divides it into bands \cite{remotesensing}. To get a monochrome camera to take hyperspectral images, a lens equipped with slits that uses diffraction to spread the wavelengths is used. The wavelengths then get spread over the sensor in one dimension, therefore pixels will in one dimension represent space and the other dimension represents bands, this is illustrated in figure \ref{fig::pixelsmonochrome}.
\begin{figure}
    \centering
    \begin{tikzpicture}
    \draw[step=0.5cm,color=gray] (-1,-1) grid (1,1) rectangle (-1,-1);

    \node at (0,1.25) {x};
    \node at (-1.25,0) {$\lambda$};

    \draw (3.5,0) ellipse (.1cm and .5cm);

    \draw[red, dashed] (3.2, 0) -- (1.2, -.75);
    \draw[green, dashed] (3.2, 0) -- (1.2, -.25);
    \draw[blue, dashed] (3.2, 0) -- (1.2, .25);
    \draw[violet, dashed] (3.2, 0) -- (1.2, .75);

    \draw[dashed, ->] (6, 0) -- (3.8, 0);
    \end{tikzpicture}
    \caption{Monochrome sensor}
    \label{fig::pixelsmonochrome}
\end{figure}

\FloatBarrier
There are two common methods for hyperspectral imaging in satellite imaging, it is whiskbroom scanning and pushbroom scanning, on this satellite push broom scanning is used. Figure The principle of pushbroom scanning is shown in figure \ref{pushbroom}. The space dimension of the sensor is perpendicular to the direction of movement and images are captured continuously.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/pushbroom.png}
	\caption{Push broom\cite{seelye}}
	\label{pushbroom}
\end{figure}
\FloatBarrier

Area images in different wavelengths is obtained by stitching together the continuously captured frames, the spatial dimension of the sensor and all the captured frames becomes X and Y spatial dimensions shown in figure \ref{areacube}, and the band dimension of the sensor is represented in the Z direction.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\textwidth]{images/cube(2).png}
	\caption{Hyperspectral datacube\cite{remotesensing}}
	\label{areacube}
\end{figure}

\FloatBarrier

A common way to store hyperspectral images is in a cube format. There are three types that are often used, these are Band Interleaved by Line(BIL), Band Interleaved by Pixel(BIP) and Band SeQuential(BSQ), as presented in figure \ref{cubeformats}.

\begin{figure}[!htb]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
	  %\rule{\linewidth}{\dimexpr 2\linewidth+2\baselineskip+6pt}
	  \includegraphics[width=\textwidth]{images/bsq.png}
       \caption{BSQ}
       \label{subfig-1:dummy}
     \end{subfigure}
     \hfill
     \begin{minipage}[b]{0.49\textwidth}
       \begin{subfigure}[b]{\linewidth}
	    %\rule{\linewidth}{\linewidth}
	    \includegraphics[width=\textwidth]{images/bil.png}
         \caption{BIL}
         \label{subfig-2:dummy}
       \end{subfigure}\\[\baselineskip]
       \begin{subfigure}[b]{\linewidth}
	    %\rule{\linewidth}{\linewidth}
	    \includegraphics[width=\textwidth]{images/bip.png}
         \caption{BIP}
         \label{subfig-3:dummy}
       \end{subfigure}
     \end{minipage}
     \caption{Cube formats\cite{hsi}}
     \label{cubeformats}
   \end{figure}

BIL stores the pixels from X spatial direction consecutively for each band in each row and the y spatial direction represents the rows. For BIP the y spatial direction also represents the rows, but within the rows, all the bands for one pixel in X spatial direction is placed consecutively, then the same for the next pixel. BSQ is more different than the other two, here all the pixels in X spatial direction from one band is represented in the columns. The Y spatial direction is represented in consecutive rows for one band, then the next band is represented.

\FloatBarrier
\section{Satellite platform}
\label{chap::satplatform}
Figure \ref{fig::platform} shows how the satellite platform looks like, it is delivered by Nano Avionics and is called M6P (Multi Purpose 6U). All systems and subsystems uses the CSP protocol (Cubesat Space Protocol).


CSP runs on FreeRTOS, POSIX and pthread based operating systems and it is written in C\cite{cspgit}. It has a service-oriented network topology split into two segments, ground segment and space segment. The space segment is the platform shown on Figure \ref{fig::platform}, and the ground segment is in the station on the ground communicating with the satellite. A CSP node is run on every module in the network and each node can run multiple services, every service can also recieve different commands and data. Communication can be done freely between the modules in the form of CSP packets.

\begin{figure}
\begin{tikzpicture}

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (PC) at (0,0) {PC};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (Sband) at (4,3) {Sband};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (SDR) at (4,-0) {SDR};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (HSI) at (8,-0) {HSI};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (UHF) at (4,-3) {UHF};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (EPS) at (8,-3) {EPS};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (FC) at (12,-3) {FC};

\path [draw, thick] (1.15, .35) --  (2, .35);
\path [draw, thick] (2, .35) --  (2, 1.5);
\path [draw, thick] (2, 1.5) --  (4, 1.5);
\path [draw, thick] (4, 1.5) -- (Sband);

\path [draw, thick] (PC) --  (2, 0);
\path [draw, thick] (2, 0) --  (2, -1.5);
\path [draw, thick] (2, -1.5) --  (8, -1.5);
\path [draw, thick] (4, -1.5) -- (SDR);
\path [draw, thick] (8, -1.5) -- (HSI);

\path [draw, thick] (1.15, -.35) --  (1.65, -.35);
\path [draw, thick] (1.65, -.35) --  (1.65, -4.5);
\path [draw, thick] (1.65, -4.5) -- (12, -4.5);
\path [draw, thick] (4, -4.5) -- (UHF);
\path [draw, thick] (8, -4.5) -- (EPS);
\path [draw, thick] (12, -4.5) -- (FC);

\node[] at (2.2, -4.2) {CAN};
\node[] at (2.6, -1.2) {CAN};
\node[] at (2.6, 1.8) {SPI};
%\path [draw, ->, -latex, dashed] (tekst) -- (6.4, -1.1);
\end{tikzpicture}
\caption{Satellite platform}
\label{fig::platform}
\end{figure}

CSP packets get packed and sent between the segments by CAN bus or networks with the UHF module. The platform has two CAN busses, where on one bus the payloads are connected and on the other bus the satellite operation systems are connected. PC (Payload Controller) directs CSP packets between the CAN buses, and buffers mission data from the payloads and sends it to the S-band which in turn sends data to the ground station. The FC (Flight Computer) logs telemetry data and handles attitude determination and control. EPS (Electrical Power System) delivers power to the satellite. The power source is Lithium-Ion batteries that are charged with the solar panels, the batteries has a capacity of 161Wh.

Figure \ref{fig::onboardprocessing} shows the HSI payload, consisting of a system on chip with a processing system and programmable logic. The red arrows show the data pipeline, here the frames gets captured by the camera, and transmitted over ethernet network to the processing system, which bins, stores and sends data with a DMA accellerator called CubeDMA\cite{cubedmapaper} to the programmable logic. In the programmable logic super resolution, target detection and a compression algorithm is applied. The data is sent back to memory and forwared over CAN (Controller Area Network) bus with the CSP protocol.

The gray boxes in Figure \ref{fig::onboardprocessing} shows the stages of the processing pipeline which are the focus of this thesis. In the processing system, a Embedded Linux OS is built by Petalinux tools, this is set up to handle memory for DMA. The camera communication, data processing and CubeDMA communication are handled with applications written in C++, that is set up to run as CSP services. The lossless compression and CubeDMA implementations are uploaded from the OS's boot image to the FPGA.

\begin{figure}
\includegraphics[width=\textwidth]{images/onboardprocessing.png}
\caption{On-board processing\cite{ntnuinternal}}
\label{fig::onboardprocessing}
\end{figure}

\section{Ueye camera}
\label{chap::ueye}
IDS Ueye cameras come in three different series; U3, GV and UI. U3 use USB connections and GV use Gigabit Ethernet connections\cite{propids}. U3 and GV use the standards USB3 Vision and GigE Vision as communication protocol, both of these are closed protocols developed by the Automated Imaging Association (AIA), which is a trade group for the machine vision industry \cite{visionstd}. The UI series comes with either a USB connection or a GigE connection. These cameras use a proprietary protocol developed by IDS. To communicate with these cameras IDS provides a closed source driver that uses a C interface for communication and can be used for both USB and GigE.

The camera chosen to be used on the satellite is a UI-5260CP Rev. 2 camera. This is a GigE camera, it has a Sony IMX249 CMOS monochrome sensor, maximun 47.0 fps and a 1936$\times$1216 resolution. It has integrated FPGA that can be used to do various pre processing, like cropping out a area of interest (AOI), subsampling and gain boost \cite{ueyemanual}.

To capture a image, the camera can run in different operating modes: hardware trigger mode, software trigger mode or freerun mode. In freerun mode, the framerate and numbers of images to capture gets specified first, then a command from software gets sent to start a image capture sequence. In SW- and HW-trigger mode one image gets captured when the camera receives a signal from either SW or HW respectively. When in trigger mode, the camera goes into standby after it has captured a image to wait for the next trigger signal. In standby the image sensor is switched off. Figure \ref{fig:triggerHW} and \ref{fig:freerun} shows a timing diagram of the operating modes.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/hwTrigger.PNG}
	\caption{Trigger mode \cite{ueyemanual}}
	\label{fig:triggerHW}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/trigger2.png}
	\caption{Freerun \cite{ueyemanual}}
	\label{fig:freerun}
\end{figure}

The camera has a 128MB image memory, this memory is used by the camera as a buffer when transferring images, there is also a user memory of 128KB. It has a I/O connector with 8 pins (4 pairs), one pair is for power, one for camera flash, one for HW-trigger and one for GPIO. The GPIO pins can be used as trigger input, flash, set as output low or high, signal level polling and serial interface. When used as a serial interface, files can be transferred to and from user memory on the camera. A overview of the pin assignment is shown in table \ref{tab::iohirose}. The GigE connection has up to 1 Gbps transfer rate while cameras that use USB2.0 has a rate of 480 Mbps and USB3.0 can achieve rates up to 5Gbps.

\begin{table}[]
\centering
\caption{Pin assignment camera}
\label{tab::iohirose}
\begin{tabular}{|l|l|}
\hline
\rowcolor{Lightgray} \textsf{Pin} & \textsf{Description}         \\ \hline
1   & Ground              \\ \hline
2   & Flash output        \\ \hline
3   & General Purpose I/O \\ \hline
4   & Trigger input       \\ \hline
5   & Flash output        \\ \hline
6   & General Purpose I/O \\ \hline
7   & Trigger input       \\ \hline
8   & Input power         \\ \hline
\end{tabular}
\end{table}
\FloatBarrier
The driver comes in versions for Windows, Linux and Embedded Linux. It has three different ways of capturing images; bitmap mode, direct3D mode and openGL mode. In bitmap mode, the images get written to a specified place in memory, then they can be processed or fetched from there. In direct3D and openGL mode, the images get written to GPU memory, the GPU must support either direct3D or openGL to use any of these modes. uEye cameras can capture images with bit-depths of 8, 10 or 12. The driver only allows bit-depths of 8 to be transferred over USB2.0 connections, but allows 8, 10 or 12 to be transferred over USB3.0 and GigE connections.

Figure \ref{figure:camdir} shows how the sensor is oriented in relation to the direction of movement and in which order the pixels are stored in memory. Frames are stored row by row from the sensor, this makes the raw data stored in memory be in BIP order.

\begin{figure}[h!]
\centering
\begin{tikzpicture}
\draw[step=0.5cm,color=gray, fill=hue] (-1,-1) grid (1,1) rectangle (-1,-1);
\node at (-0.75,+0.75) {1};
\node at (-0.25,+0.75) {2};
\node at (+0.25,+0.75) {3};
\node at (+0.75,+0.75) {4};
\node at (-0.75,+0.25) {5};
\node at (-0.25,+0.25) {6};
\node at (+0.25,+0.25) {7};
\node at (+0.75,+0.25) {8};
\node at (-0.75,-0.25) {9};
\node at (-0.25,-0.25) {10};
\node at (+0.25,-0.25) {11};
\node at (+0.75,-0.25) {12};
\node at (-0.75,-0.75) {13};
\node at (-0.25,-0.75) {14};
\node at (+0.25,-0.75) {15};
\node at (+0.75,-0.75) {16};

\node at (0,1.25) {$\lambda$};
\node at (-1.25,0) {x};

\path [draw, ->, thick, -latex] (-1,-1.25) -- (1,-1.25);
\path [draw, ->, thick, -latex] (1,-1.25) -- node [below] {Directions of movement} (-1,-1.25);
\end{tikzpicture}

\captionof{figure}{Sensor orientation and pixel readout}
\label{figure:camdir}

\end{figure}

To use the C interface, the camera has to be initialized first with the \textsf{is\_InitCamera} function shown below. The arguments to the function are a pointer to a camera handle, which is unique for every camera connected, and a pointer to the window used by Direct3D, if Direct3D is not used this argument is NULL. To exit the camera, only the camera pointer is needed.

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_InitCamera} (HIDS* \textbf{phf}, HWND \textbf{hWnd})}]


\textbf{phf} - Pointer to camera handle

\textbf{hWnd} - Pointer to Direct3D

\end{tcolorbox}

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_ExitCamera} (HIDS \textbf{hCam})}]

\textbf{hCam} - Camera handle

\end{tcolorbox}






%\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=My nice heading]This is another \textbf{tcolorbox}.\tcblowerHere, you see the lower part of the box.\end{tcolorbox}

To set the size of the image, three functions are used.
\begin{itemize}
    \item \textsf{is\_SetColorMode}
    \item \textsf{is\_ImageFormat}
    \item \textsf{is\_AOI}
\end{itemize}
The \textsf{is\_SetColorMode} function tells the driver and the camera what type of images it is supposed to capture, wheter it's RGB, gray scale, 8-, 10-, 12-bit, packed or unpacked. The mode argument is a enum which corresponds to one colormode, all possible different color modes can be found in the camera manual.

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_SetColorMode} (HIDS \textbf{hCam}, INT \textbf{Mode})}]

\textbf{Mode} - Colormode enum parameter for predefined colormodes

\end{tcolorbox}

Function \textsf{is\_ImageFormat} can query possible resolutions, query if a custom AOI on the sensor is possible and set a resolution for the sensor. When the driver sets the resolution with this function the driver can adjusts the AOI, subsampling and sensot binning to get the highest possible framerate. When the set resolution command is passed to the function, the pParam pointer points to a integer that defines what imageformat it is going to be set to.

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_ImageFormat} (HIDS \textbf{hCam}, UINT \textbf{nCommand}, void \textbf{*pParam}, UINT \textbf{nSizeOfParam})}]

\textbf{nCommand} - The command call for what operation the function is supposed to do

\textbf{*pParam} - Pointer to the parameters that goes along with the command

\textbf{nSizeOfParam} - Size of the parameters pointed to by pParam
\end{tcolorbox}

The function \textsf{is\_AOI} can be used to set a AOI positioned on the sensor array, get the minimum and maximum possible size and position of the AOI and get the current position and size of AOI. A struct with x and y position, width and height of the AOI is passed as a parameter to the function.

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_AOI} (HIDS \textbf{hCam}, UINT \textbf{nCommand}, void* \textbf{pParam}, UINT \textbf{nSizeOfParam})}]

\textbf{nCommand} - The command for what operation the function is supposed to do

\textbf{*pParam} - Pointer to the parameters that goes along with the command

\textbf{nSizeOfParam} - Size of the parameters pointed to by pParam
\end{tcolorbox}

The displaymode is set with the \textsf{is\_SetDisplayMode} function. The \textsf{is\_CaptureVideo} sets the camera in freerun mode, where the wait parameter tells the camera if imagecapturing should start immediately, in a given time or when it gets a signal to start.

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_SetDisplayMode} (HIDS \textbf{hCam}, INT \textbf{Mode})}]

\textbf{Mode} - What displaymode to set

\textbf{*pParam} - Pointer to the parameters that goes along with the command

\textbf{nSizeOfParam} - Size of the parameters pointed to by pParam
\end{tcolorbox}

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_CaptureVideo} (HIDS \textbf{hCam}, INT \textbf{Wait})}]

\textbf{Mode} - Timeout value
\end{tcolorbox}

To allocate memory for a frame, the \textsf{is\_AllocImageMem} is used. This takes the width, height and pixel depth as input arguments and returns a pointer to the allocated memory and the drivers ID of that memory as output arguments.

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_AllocImageMem} (HIDS \textbf{hCam}, INT \textbf{width}, INT \textbf{height}, INT \textbf{bitspixel}, char** \textbf{ppcImgMem}, INT* \textbf{pid})}]

\textbf{width} - Width of image

\textbf{height} - Height of image

\textbf{bitspixel} - Bits er pixel

\textbf{ppcImgMem} - Pointer to memory

\textbf{pid} - ID of memory

\end{tcolorbox}

The driver keeps a buffer of image locations, this buffer is a ringbuffer that it cycles through when it stores images. The \textsf{is\_AddToSequence} is used to add memory locations to this buffer. The queue must be initialized before it can be used, that is done with \textsf{is\_InitImageQueue}.

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_AddToSequence} (HIDS \textbf{hCam}, char* \textbf{pcImgMem}, INT \textbf{pid})}]

\textbf{pcImgMem} - Pointer to memory

\textbf{pid} - ID of memory
\end{tcolorbox}

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={INT \textsf{is\_InitImageQueue} (HIDS \textbf{hCam}, INT \textbf{nMode})}]

\textbf{nMode} - Queue mode. As of now, only ringbuffer is supported.
\end{tcolorbox}






\section{Zynq7000}
% Zynq 7000 is a family of chips made by Xilinx, figure \ref{fig::soc} shows the architecture of the chip. The chip consists two main parts, the processing system(PS) and the programmable logic(PL). The programmable logic can be either Artix 7 or Kintex 7 PL, of varying sizes.

% \begin{figure}[h!]
% 	\centering
% 	\includegraphics[width=.95\textwidth]{images/zynq-mp-core-dual.png}
% 	\caption{Zynq-7000\cite{zynq7000doc}}
% 	\label{fig::soc}
% \end{figure}

% The chip is going to be used with a System On Module(SOM) called Picozed but for development a Zedboard is used\cite{zedboard}\cite{zynq700}, the boards are shown on figure \ref{fig::som}. Both boards has Ethernet, USB 2.0 and JTAG connections, and SD card slots. They have DDR 3 memory and QSPI flash, here the Zedboard has 512 MB DDR3 256 MB QSPI, while the Picozed has 1GB DDR3 and 128MB QSPI. The Picozed also has a 4GB eMMC memory.

% \begin{figure}[!h]
%      \centering
%      \begin{subfigure}[b]{0.49\textwidth}
% 	  \includegraphics[width=\textwidth]{images/Zedboard.png}
%       \caption{Zedboard\cite{avnetzed}}
%       \label{subfig-1:zed}
%      \end{subfigure}
%      \hfill
%   \begin{subfigure}[b]{0.49\textwidth}

% 	  \includegraphics[width=\textwidth]{images/Picozed.png}
%       \caption{Picozed\cite{avnetzed}}
%       \label{subfig-2:pico}
%      \end{subfigure}
%      \caption{Boards}
%      \label{fig::som}
%   \end{figure}


% The processing system is divided into four modules; Application processor unit (APU), Memory interfaces, I/O peripherals (IOP) and Interconnects. There are controllers for many different memory interfaces and static memory. There are IOP's for UART, I2C, USB 2.0, GPIO, GigE and CAN. The interconnects are internal interconnects in the PS and between PS and PL, the architecture for this is a AMBA bus.

%In space a board with the Zynq-7000 chip called Picozed is going to be used, but
%during development a board with the same chip, called Zedboard have mostly been used. This is because the smallsat lab only have access to one Picozed board, that has to be shared among all the members, but there are many zedboards available, figure \ref{fig::som} show how the two boards look like.
It is intended to use a Picozed board with the Zynq-7000 chip for payload in space, but during development a smaller Zedboard board with the same chip has mostly been used, figure \ref{fig::som} show how the two boards look like.

\begin{figure}[!h]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
	  \includegraphics[width=\textwidth]{images/Zedboard.png}
      \caption{Zedboard\cite{avnetzed}}
      \label{subfig-1:zed}
     \end{subfigure}
     \hfill
  \begin{subfigure}[b]{0.49\textwidth}

	  \includegraphics[width=\textwidth]{images/Picozed.png}
      \caption{Picozed\cite{avnetzed}}
      \label{subfig-2:pico}
     \end{subfigure}
     \caption{Processing boards}
     \label{fig::som}
  \end{figure}

Both boards has Ethernet, USB 2.0 and JTAG connections, and SD card slots. They have DDR 3 memory and QSPI flash, here the Zedboard has 512 MB DDR3 256 MB QSPI, while the Picozed has 1GB DDR3 and 128MB QSPI. The Picozed also has a 4GB eMMC memory\cite{zedboard}\cite{zynq700}.

The Zynq-7000 chip is a family of SoCs (System on Chips) that has a Processing system (PS) and a Programmable Logic (PL) that come in different sizes and with different features. The chips available for the zedboard and Picozed have a CPU frequency of 667MHz with programmable logic of differrent sizes\cite{zynq700}. Figure \ref{fig::soc} shows how the architecture of the chip looks like.

\begin{figure}[h!]
	\centering
	\includegraphics[width=.95\textwidth]{images/zynq-mp-core-dual.png}
	\caption{Zynq-7000 architecture\cite{zynq7000doc}}
	\label{fig::soc}
\end{figure}

\subsection{PS}

There are many different I/O peripherals, memory interfaces and a APU (Application Processing Unit) in the processing system. The APU consists of a ARM Cortex-A9 processor, this uses Armv7 architecure and comes in either single core or dual core, in this project, the dual core is used.  It has 32KB instruction and 32 KB data L1 caches and 512 KB of shareable L2 cache with a snoop controller. It has a partially out-of-order pipeline, DMA controller, FPU unit and a SIMD co-processor.

\subsubsection*{NEON}
The SIMD processor uses NEON architecture, which is a architecture developed by ARM. SIMD processors perform the same instructions on multiple data simultaneously. To do this the data is placed contiguously in registers, the data fields in the registers are referred to as lanes in NEON\cite{neonguide}. The operations are performed on two registers in corresponding lanes, Figure \ref{fig::simdex} shows how a SIMD add operation is performed on a four lane vector.

\begin{figure}[h!]
	\centering
	\includegraphics[width=.7\textwidth]{images/simdex.png}
	\caption{Four lane vector operation\cite{neonguide}}
	\label{fig::simdex}
\end{figure}

This architecture has sixteen 128-bit vectors, and the operations that are supported for integer operations are; 16x8-bit, 8x16-bit, 4x32-bit, 2x64-bit, and for floating point operations; 8x16-bit, 4x32-bit, 2x64-bit. To use the NEON architecture, assembly code can be written or compiler intrinsics can be used, these intrinsics can be either wrappers for single assembly instructions or multiple assembly instructions. They are available for Arm Compilers, gcc and LLVM compilers\cite{armcortex}\cite{armneon}.

Intrinsics exist for addition, multiplication, loading and unloading of registers and comparison operations. In a comparison operation two vectors get compared lanewise, whree min or max values gets placed in their respective destination registers. There exists instructions for operations where lanes get rearranged within a register or between registers, these are called shuffle instructions. Three of them is called zip-, transpose- and reverse operations.
\cite{neonshuffle}

Figure \ref{fig::reverseop} shows how a reverse operation in NEON can look like. N-bit lanes can be reversed in M-bit sub elements within a vector. As shown in the example, \textit{VREV16.8 d0 d1}, reverses each 8-bit pair in the 4 sub elements of 16-bit within the vector, and \textit{VREV32.8 d0 d1} reverses four 8-bit elements within 32-bit sub elements.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/reverse.png}
	\caption{Reverse operation NEON\cite{neonshuffle}}
	\label{fig::reverseop}
\end{figure}

Figure \ref{fig::transposeop} shows the transpose operation. This operation switches two diagonaly located lanes of two vectors.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.65\textwidth]{images/transpose.png}
	\caption{Transpose operation NEON\cite{neonshuffle}}
	\label{fig::transposeop}
\end{figure}

Figure \ref{fig::zipop} shows a zip operation. This operation interleaves the lanes of two source vectors into the same source vectors. Operations for un-zipping also exists.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{images/zip.png}
	\caption{Zip operation NEON\cite{neonshuffle}}
	\label{fig::zipop}
\end{figure}

\subsection{Booting}
Zynq has on chip memory (OCM), that is used for booting, the memory is divided into two parts; ROM and RAM, both have 256KB each. Figure \ref{fig::bootorder} shows the different stages of the booting process.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/bootorder.png}
	\caption{Boot stages\cite{zynqbook}}
	\label{fig::bootorder}
\end{figure}

The first stage, stage 0 that is, gets initiated when the chip receives a power on signal, this typically happens when the power switch on the board goes from off to on or when the system reboots. The first thing this stage does is to read the boot mode signals to see where to load the boot source from, it can either be JTAG, QSPI, flash or SD card. Different boot mode signals can be set with jumpers on pin MI02-MI06 on the board. When it knows where to boot from, the First Stage Boot Loader(FSBL) gets loaded from there and into RAM in OCM, then it checks if it is a valid image and if it is, it hands over control to the FSBL.

FSBL's purpose is to load the seconds stage boot loader (SSBL). It initializes the CPU, programs the FPGA, loads the Second Stage Boot Loader into main memory, and if there are any user applications here they are loaded into memory.

The SSBL executes the user application if there are any. Then loads a compressed image of the OS into memory and decompresses it, which in this case will be the kernel image. A device tree and a ramdisk file is also loaded with the kernel. The device tree is used by the kernel to see what hardware is available to it and the ramdisk is used by the kernel to help load the root file system.


Figure \ref{fig::bootfiles} shows the files involved in the boot process. FSBL, FPGA bitstream and SSBL must be packaged into one file called BOOT.BIN, and the device tree, ramdisk and kernel image usually gets packaged into a imge file called image.ub.

 \begin{figure}[h!]
	\centering
	\includegraphics[width=.60\textwidth]{images/bootfiles.png}
	\caption{Boot files \cite{zynqbook}}
	\label{fig::bootfiles}
\end{figure}




\subsection{Communication}
\subsubsection*{CAN}
In CAN (Controller Area Network) networks there is no bus master, the data gets sent in many short messages and is broadcasted to the entire network \cite{canbus}. Bus access is event driven and non destructive, if one node has access to the bus and another node requests the bus, the requesting node must wait for the currently accessing node. Arbitration is done with priority and randomly for nodes with the same priority. The physical layer consists of two wires, and the data rate is expected to be 416 Kbps for this satellite. The bus also has the ability to self diagnose and repair data errors.
\subsubsection*{GigE}
The Gigabit Ethernet connection is a 1000BASE-T connection, this means the data rate is 1000Mbps and that the physical connection is four pairs of twisted wires\cite{ueyemanual}. The communication protocol used by the camera is UDP, which is a fast but unreliable compared to other protocols used over ethernet connections. UDP sends data in packets with the IP address of the destination in the header\cite{udp} and no form of handshaking, in this way, if packets gets dropped, the sender will not know and data errors may occur.
% this is why it's considered more unreliable\cite{udp}.
\subsubsection*{AMBA}
The AMBA bus uses the AXI (Advanced eXstensible Interface) bus standard, this is developed by ARM. In this standard two nodes can occupy the bus at the same time, the initiator of the communication is the master and the other node is the slave. The data width can be either 32-bit or 64-bit.

\subsection{Embedded Linux}

In Linux, the OS is called the Kernel\cite{operatingsystems}. Memory is divided into user space and kernel space, where user space is where the user applications is running and kernel space is where the kernel is running. Kernel space usually has full access to hardware and system resources. While user space applications has limited privileges, but has access to some of the kernels functions through system calls.

The Linux kernel is modular monolithic. Modular meaning that it consists of a structure of modules that is linked together, they are dynamically linked i.e. modules can be linked and un-linked at runtime. Monolithic meaning that it runs as a single process with a single address space and all modules have access to each others internal data structures and routines. Figure \ref{fig::lunuxkern} shows the division between user space and kernel space, with some of the common modules of a kernel.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/linkern.PNG}
	\caption{Linux kernel\cite{operatingsystems}}
	\label{fig::lunuxkern}
\end{figure}

\FloatBarrier
\subsection{Memory}

Figure \ref{fig::memhir} shows how memory in modern computer systems is arranged. The design is hierarchical, where the fastest memory is closest to the CPU, the memory closer to the CPU is also smaller in size. Both L1 and L2 cache are usually built into the CPU chip, but L1 is smaller and closest to the CPU. Main memory are also known as RAM (Random Access Memory) and data that have to be manipulated in some way and applications that are running on the CPU are usually kept here. L1, L2 and RAM are all volatile memory. At the bottom of the hierarchy is storage, this is the biggest memory in size but also the slowest, this memory is not volatile, so data that are kept here is permanently stored. Storage is often a magnetic disk or flash-based memory.

Smaller memories are usually faster and memory closer to the CPU is accessed faster. The reason why having small fast memory close to the CPU is because of the principle of temporal and spatial locality. Spatial locality is the principle that when data are accessed, the data next to it is also likely to be accessed, so having data that are close to the current working data close to the CPU will make it faster to access when it is needed. Temporal locality is the principle that accessed data is likely to be accessed again in the near future, so keeping it close to the CPU provides faster access when it is needed.

 \begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/memhiera.PNG}
	\caption{Memory hierarchy\cite{comparch}}
	\label{fig::memhir}
\end{figure}

\subsubsection*{Memory partitioning}
Main memory can be partitioned with fixed partition sizes or variable partition sizes. With fixed partitions, memory is divided up into fixed length partitions known as a frames, a program is then assigned one ore more frames. With variable partition, memory is divided into segments of different sizes, the segments can either be pre defined or dynamically partitioned during runtime. Figure \ref{fig::partitt} shows how a memory can be partitioned with either fixed or variable length partitioning.

\begin{figure}
\begin{subfigure}[h]{0.49\textwidth}
\centering
\includegraphics[scale=1]{images/partitioningvariable.png}
\caption{Variable partitioning}
\label{fig::partitionvariable}
\end{subfigure}
\begin{subfigure}[h]{0.49\textwidth}
\centering
\includegraphics[scale=1]{images/partitioningfixed.png}
\caption{Fixed partitioning}
\label{fig::partitionfixed}
\end{subfigure}
\caption{Memory partitioning\cite{operatingsystems}}
\label{fig::partitt}
\end{figure}

The OS can divide programs and data either into segments or pages. When the segment scheme is used the the program is divided into segments, these segments can be different sizes with a maximum length. When the page scheme is used the programs is divided into pages that are of fixed sizes. Using the page scheme with memory divided up into frames and using small frame sizes is a convenient way to organize memory. If the pages have the same sizes as the frames, one page can be placed into one frame, this is also how it is done in Linux.
\FloatBarrier
A page table is used to keep track of what frame each page occupies, pages can then be placed anywhere in memory, making it easy to use memory more efficiently. Figure \ref{fig::pageframe} shows how three programs, A, C and D can be placed in memory and that there is no need to place pages contiguously in memory.
\FloatBarrier
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.25\textwidth]{images/pageframe.PNG}
	\caption{Paging with frames\cite{operatingsystems}}
	\label{fig::pageframe}
\end{figure}

Fragmentation is when there are unused spaces in memory which are impossible to fit a process in \cite{operatingsystems}. When the segment scheme is used, there is a risk of having a lot of unused space because new segments does not fit between the segments already in memory, this is called external fragmentation. When the paging scheme is used there is a risk of a lot of unused space in the last page at the end of each process, this is called internal fragmentation.

A page address is divided up into a page number and a offset value, Figure \ref{fig::logicaddress} shows an example of this for a logical 16-bit address. Logical addresses is a memory references that do not reference directly to physical addresses. The first 6-bits of the address represents the page, this is used by a page table to find the frame number in physical memory. The last ten bits is the offset to the data within the page/frame.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/logicaddress.PNG}
	\caption{Logical address \cite{operatingsystems}}
	\label{fig::logicaddress}
\end{figure}

%Memory management is a collaboration between HW and OS. The page table of processes are kept in memory, this results in that every memory access includes two memory accesses, one for translation from logical address to physical address and one for accessing the actual physical memory. To help prevent this a HW unit called a Translation Lookaside Buffer (TLB) is used, the TLB caches the page tables based on some caching algorithm, and if the logical address is present in the TLB, the translation to physical memory is done directly so that only one memory access is necessary.

\subsubsection*{Virtual memory}
A technique used in many modern systems is virtual memory. This is used to make it look like memory is larger than it really is. This has the advantage of making it so that lots of different data and different processes can be maintained in memory at the same time and keep processes larger than physical memory in memory. For this to be possible, their own virtual address space of logical addresses is assigned to all processes. The pages of a process can be in either memory or storage, OS places pages in either memory or storage, usually pages that is not needed is placed in storage, this is called swapping. Each process has its own page table. Figure \ref{fig::pagentry} shows how a paget table entry might look like for virtual memory. A P-bit tells if the page is present in memory, a M-bit tells if the page has been modified since the last time it was loaded from storage and the other control bits can be stuff like protection and sharing.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.65\textwidth]{images/pagentry.png}
	\caption{Page table entry\cite{operatingsystems}}
	\label{fig::pagentry}
\end{figure}
\FloatBarrier
Different page tables might be of different sizes, so a pointer to the page table is needed. Figure \ref{fig::virtophys} shows how a virtual address and page table pointer reference a physical address in memory.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/virtophys.png}
	\caption{Virtual- to physical address with page table pointer\cite{operatingsystems}}
	\label{fig::virtophys}
\end{figure}

\subsection{Memory in Linux}
Because page tables might become big, it is not favorable to keep all of the table in memory. To solve this, page tables can also be stored in virtual memory. This leads to a hierarchical page table system. For Linux a three level page table structure is used, this is shown in figure \ref{fig::linpagetable}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/threelevelpages.png}
	\caption{Linux page table structure \cite{operatingsystems}}
	\label{fig::linpagetable}
\end{figure}

The first level is the page directory, this can be found at a address stored in the page table pointer. The page directory points to a page in the middle directory, which in turn points to the actual page table, that points to a page. The virtual address is divided into three directory offset values which is used in their respective directories, and a offset for the actual page. The size of the page directory is one page while the sizes of middle directory and page table can be multiple pages long. If the hardware does not support three level page tables, the middle directory is set to be only one entry, which eliminates this level.

There are three types of addresses associated with memory in Linux, those are; User virtual addresses, Kernel virtual addresses and Kernel logical addresses. Each process has its own virtual address space which also requires their own virtual addresses, these addresses can be 32- or 64-bit long. Kernel virtual addresses can also be 32- or 64-bit long, they can be either regular virtual addresses or kernel logical addresses. The logical addresses have a one to one mapping to physical memory addresses, where the page number of the virtual address corresponds to the frame number of the physical address, and the offset in the page corresponds to the offset in the frame. A Kernel virtual address which is not a logical address can be mapped anywhere in memory.

\subsection{Kernel module}
A kernel module can either be a char driver, block driver or network driver. Char drivers are accessible through a device file in the file system and is read or written to char by char(one byte at the time), like a stream of data. Block drivers are mounted as a file system and is read or written to with blocks of data. Network drivers are similar to block devices, but differ in that they can send and receive data asynchronously on the network. In this project a char driver has been used, because it provides the functionality needed for the design, no big blocks of data has to be sent between kernel and user space, and no access to the network is needed.

Associated with kernel modules, are something called major and minor numbers. The purpose of these numbers are to identify the driver and the device. In some systems, only major numbers are used, in these systems the major number tells the kernel exactly what driver and device file it refers to (multiple devices can be created from one driver). In other systems the major number refers to what driver it is and the minor number refers to what device it is. In this system only major numbers are used. The major number can be given to the driver from the kernel or created by the driver, this has to be registered in the kernel.

Communication between the user space application and the kernel module happens through system calls from user space. Char devices requires a few structures to work with the kernel, the most important are; file operations struct, file struct, cdev struct and inode struct.

\begin{itemize}
  \item \textbf{File operations structure} This structure contain pointers to the functions to execute when user space uses system calls for this device, listing \ref{lst::fops} shows an example for one of these. When the \textit{open} system call is used on this device, the kernel knows what function to execute, because it has a pointer to the \textit{dev\_open} function.


 \begin{lstlisting}[language=C++, caption={File operations}, label={lst::fops}, numbers=none]
struct file_operations fops = {
  .open = dev_open,
  .read = dev_read,
  .write = dev_write,
  .release = dev_release,
  .mmap = dev_mmap,
	.unlocked_ioctl = dev_ioctl
};
\end{lstlisting}

\item\textbf{cdev structure}
The cdev struct is the kernels way of representing char devices. This structure contains a pointer to the file operations structure.

  \item \textbf{File structure.} This structure contains information about the device that can be used by the device and kernel. This structure is created by the OS when the device is opened and is passed to all functions used in the driver. This structure has many fields, perhaps the most important ones are show in listing \ref{lst::file}. The \textit{mode\_t} field has information on if the device is readable or writable, the file operation struct described earlier, and the private data field. The private data field can be set to point to anything, this can be helpful for many things within the driver.

   \begin{lstlisting}[language=C++, caption={File structure}, label={lst::file}, numbers=none]
struct file file_str = {
    .
    .
    mode_t f_mode;
    struct file_operations *f_op;
    void *private_data;
    .
    .
};
\end{lstlisting}

  \item \textbf{Inode structure.} Contains information about files and device files in the Linux system, this is information created and used in the OS. The most important data it contains, from a kernel module programmers point of view is the major and minor numbers
\end{itemize}

For any further information of the structure of drivers, Linux kernel or Linux memory more information can be found in the Linux Device Drivers book, free online \cite{ldd3}.
\section{Accelerators}

\subsection{CubeDMA}

The cubeDMA core is a FPGA core specialized in direct memory accessing of cube data, it can read BSQ and BIP ordered cubes from memory \cite{cubedmapaper}. It is designed to be able to read bit-depths that are not byte multiples and stored without padding. It is possible to read a cube block-wise, so that if the accelerator requires data to be streamed in blocks, one block is streamed entirety before the next block. The limitation is that the size of the blocks must be powers of two.

The cube (or block) is read from left to right, Figure \ref{fig:streamingorder} shows the order of how both BIP and BSQ cubes are streamed from memory with a cube that is divided into four blocks. The red lines indicate the order the pixels are read, the width and height are spatial dimensions and depth is the spectral dimension.

\begin{figure}[!htb]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
	  \includegraphics[width=\textwidth]{images/dmabip.PNG}
       \caption{BIP}
       \label{subfig-1:bip}
     \end{subfigure}
     \hfill
   \begin{subfigure}[b]{0.49\textwidth}

	  \includegraphics[width=\textwidth]{images/dmabsq.PNG}
       \caption{BSQ}
       \label{subfig-2:bsq}
     \end{subfigure}
     \caption{Streaming order cube DMA\cite{johanmaster}}
     \label{fig:streamingorder}
   \end{figure}

On Figure \ref{fig:cubedmacore} is a block diagram of the cubeDMA core. It consists of two parts; memory to stream channel(MM2S) and a stream to memory channel(S2MM). Both of them does as the names sugest, MM2S accesses memory and streams the data into the FPGA, and S2MM streams data from the FPGA and stores it in memory.


\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/cubedmacore.PNG}
    \caption{CubeDMA core\cite{johanmaster}}
    \label{fig:cubedmacore}
\end{figure}

The MM2S channel consists of three parts; The datamover/tinymover, unpacker and controller. Datamover/tinymover is responsible for reading directly from memory over the bus. The datamover is a Xilinx IP \cite{datamover} and it is used for BIP transfers in this design, the tinymover is designed by the creator of the cubeDMA core and is used for BSQ transfers. The datamover or tinymover sends the lines read from memory to the unpacker. The unpacker shifts the data into alignment and correct word sizes. Figure \ref{fig:unpacker} shows how a 64-bit word of unaligned 12-bit data from memory is shifted and placed into words containing four 12-bit samples. The controller manages the datamover and unpacker during transfer and also outputs a control stream that an accelerator might need. The information in the control stream is; whether a sample is the last pixel in a block, if a sample are in the last column and if a sample is in the last row.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/unpacker.PNG}
    \caption{Unpacker cube DMA \cite{johanmaster}}
    \label{fig:unpacker}
\end{figure}

The S2MM channel works in a similar way as the MM2S channel, only in opposite order. The packer recieves data from a stream and packs it into correct word sized and sends them to the datamover. The datamover writes the data sequentially to memory over the axi bus. While the controller manages the datamover and packer.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\textwidth]{images/packer.PNG}
%     \caption{Packer cube DMA \cite{johanmaster}}
%     \label{fig:packer}
% \end{figure}
\FloatBarrier
The controllers reads and writes to memory mapped register. These registers must be set up to execute a transfer, table \ref{tab::mm2s} and \ref{tab::S2MM} shows theses registers. To implement the cubeDMA a few parameters must be chosen, those are shown in figure \ref{tab::dmaparameters}

\FloatBarrier
\begin{table}[]
\centering
\caption{MM2S registers}
\label{tab::mm2s}

\begin{tabular}{|l|l|l|}
\hline
\rowcolor{Lightgray} \textsf{Field} & \textsf{Description} & \textsf{Bits}  \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x00}                                                                                                                                   \\ \hline
Start                     & \begin{tabular}[c]{@{}l@{}}Starts when this bit transitions \\ from 0 to 1\end{tabular}                                           & 0     \\ \hline
Blockwise mode            & Cube is read in blocks                                                                                                            & 2     \\ \hline
Planewise mode            & Cube is read in plains                                                                                                            & 3     \\ \hline
Error IRQ enable          & IRQ when error occurs                                                                                                             & 4     \\ \hline
Completion IRQ enable     & IRQ when transfer completed                                                                                                       & 5     \\ \hline
Number of plane transfers & ceil(depth/C\_MM2S\_NUM\_COMP)                                                                                                    & 8-15  \\ \hline
Start offset              & Number of components start offset                                                                                                 & 16-23 \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x04}                                                                                                                                   \\ \hline
Transfer done             & Indicates that transfer has completed                                                                                             & 0     \\ \hline
Error code                & \begin{tabular}[c]{@{}l@{}}Indicates error condition(s) that \\ occured during transfer\end{tabular}                              & 1-3   \\ \hline
Error IRQ flag            & \begin{tabular}[c]{@{}l@{}}On Read: 1 when IRQ triggered due to error\\ On Write: Writing 1 clears the IRQ flag\end{tabular}      & 4     \\ \hline
Completion IRQ flag       & \begin{tabular}[c]{@{}l@{}}On Read: 1 when IRQ triggered due to completion\\ On Write: Writing 1 clears the IRQ flag\end{tabular} & 5     \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x08}                                                                                                                                   \\ \hline
Base address              & Address of the first component in the HSI cube                                                                                    & 0-31  \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x0C}                                                                                                                                   \\ \hline
Width                     & The width of the HSI cube                                                                                                         & 0-11  \\ \hline
Height                    & The height of the HSI cube                                                                                                        & 12-23 \\ \hline
Depth (low)               & \begin{tabular}[c]{@{}l@{}}Lower 8 bits of the depth / \\ number of planes of the HSI cube\end{tabular}                           & 24-31 \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x10}                                                                                                                                   \\ \hline
Block width               & log2 of block width in pixels                                                                                                     & 0-3   \\ \hline
Block height              & log2 of block height in pixels                                                                                                    & 4-7   \\ \hline
Depth (high)              & \begin{tabular}[c]{@{}l@{}}High 4 bits of the depth / \\ number of planes of the HSI cube\end{tabular}                            & 8-11  \\ \hline
Last block row size       & \begin{tabular}[c]{@{}l@{}}Size of one row in the last block \\ in each row of blocks\end{tabular}                                & 12-31 \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x14}                                                                                                                                   \\ \hline
Row size                  & Number of components in one row of the cube                                                                                       & 0-19  \\ \hline
\end{tabular}
\end{table}

\FloatBarrier
\begin{table}[]
\centering
\caption{S2MM registers}
\label{tab::S2MM}

\begin{tabular}{|l|l|l|}
\hline
\rowcolor{Lightgray}\textsf{Field}  & \textsf{Description} & \textsf{Bits} \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x20}                                                                                                                              \\ \hline
Start                 & Core starts transfer when this bit transitions from 0 to 1                                                                        & 0    \\ \hline
Error IRQ enable      & Trigger IRQ when error condition arises                                                                                           & 4    \\ \hline
Completion IRQ enable & Trigger IRQ when transfer is complete                                                                                             & 5    \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x24}                                                                                                                              \\ \hline
Transfer done         & Indicates that transfer has completed                                                                                             & 0    \\ \hline
Error code            & Indicates error condition that occured during transfer                                                                            & 1-3  \\ \hline
Error IRQ flag        & \begin{tabular}[c]{@{}l@{}}On Read: 1 when IRQ triggered due to error\\ On Write: Writing 1 clears the IRQ flag\end{tabular}      & 4    \\ \hline
Completion IRQ flag   & \begin{tabular}[c]{@{}l@{}}On Read: 1 when IRQ triggered due to completion\\ On Write: Writing 1 clears the IRQ flag\end{tabular} & 5    \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x28}                                                                                                                              \\ \hline
Base address          & Address of where to store data                                                                                                    & 0-31 \\ \hline
\rowcolor{Lightgray}\multicolumn{3}{|l|}{Register 0x2C}  \\ \hline
Received length       & \begin{tabular}[c]{@{}l@{}}Number of bytes received from start \\ of transfer until TLAST was asserted\end{tabular}               &      \\ \hline
\end{tabular}
\end{table}


\begin{table}[]
\centering
\caption{CubeDMA parameters }
\label{tab::dmaparameters}

\begin{tabular}{|l|l|}
\hline
\rowcolor{Lightgray}\textsf{Name}& \textsf{Description}     \\ \hline
C\_MM2S\_AXIS\_WIDTH & Bit width axi stream from memory             \\ \hline
C\_MM2S\_COMP\_WIDTH & Bit width for one sample from memory         \\ \hline
C\_MM2S\_NUM\_COMP   & Number of components in one word from memory \\ \hline
C\_S2MM\_AXIS\_WIDTH & Bit width axi stream to memory               \\ \hline
C\_S2MM\_COMP\_WIDTH & Bit width for one sample to memory           \\ \hline
C\_S2MM\_NUM\_COMP   & Number of components in one word to memory   \\ \hline
C\_TINYMOVER         & Use of tinymover or datamover               \\ \hline

\end{tabular}
\end{table}
\FloatBarrier
\subsection{Compression}

CCSDS-123 is a image compression algorithm developed by the Consultative Committee for Space Data Systems(CCSDS), this is a committee consisting of many member agencies that meet to discuss common data system problems. For extensive information on algorithm, a document describing the standard is available for anyone \cite{ccsds123}. CCSDS-123 is lossless compression algorithm that is very well suited for hyperspectral images, due to the cube like nature of the data. The compressed image is of varying length depending on the input samples.The compression is done in two stages, predictor stage and encoder stage, shown in figure \ref{fig:blockcompr}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/blockCompr.PNG}
    \caption{Compressor CCSDS-123 \cite{ccsds123}}
    \label{fig:blockcompr}
\end{figure}

The input samples to the predictor is a fixed size of D bits, where D is $2\leq D \leq 16$. The predictor stage uses adaptive linear prediction. This means that it uses the sample neighborhood in x, y spatial direction and z spectral direction to predict the value of a sample. The output from this stage is the difference in the predicted value of a sample and the actual value.

The output of the predictor is the input of the encoder. The encoder can either be sample adaptive entropy or block adaptive entropy encoder. Sample adaptive entropy coding uses statistics to assign a variable length code word to each sample. Block adaptive entropy coding divides the samples into blocks, and use independently and adeptly selected encoding methods for each block. This project uses the sample adaptive entropy encoder. The output is a image with a header and a body, where the body are the encoded mapped prediction residuals, and the header consists of image metadata, predictor metadata and entropy coder metadata.

\subsubsection*{Predictor}

The predictor uses samples from previous bands and the neighborhood within the same band to make the predictions, a typical prediction neighborhood is shown in figure \ref{fig:predictionneigh} Calculation of local sum, central local difference and directional local difference uses neighborhoods like these. The local sum, central local difference and directional local difference is then used to make the predicton calculations.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/predictionNeighborhood.PNG}
    \caption{Prediction neighborhood CCSDS-123 \cite{ccsds123}}
    \label{fig:predictionneigh}
\end{figure}

\subsubsection*{Local sum}
The local sum is a weighted sum of neighboring samples in the x and y direction within the same band. It can either be a column- or neighbor-oriented local sum, this defines how the neighborhood of samples look like. The differences between the local sum and neighborhood of pixels is called central local differences and directional local differences and are put into a local difference vector, that also includes P previous local differences from previous bands. The local difference vector is used to produce a dot product with a corresponding weight vector.

\subsubsection*{Calculating weights}
\label{chap:weight}

For each spectral band the initial weight vector components is assigned as in equation \ref{eq:initweight}. $\Omega$ is a user defined parameter in the range $4\leq\Omega\leq19$.

\begin{equation}
    \label{eq:initweight}
    \omega_{z}^{1}(1) = \frac{7}{8}2^{\Omega}, \omega_{z}^{i}(1) = \lfloor\frac{1}{8}\omega_{z}^{i-1}(1)\rfloor, i=2,3,...,P_{z}^{*}.
\end{equation}
And the direction weight components is $\omega_{z}^{N}(1) = \omega_{z}^{W}(1) = \omega_{z}^{NW}(1) = 0$

For each prediction calculation, the weight vector is updated, the new vector is found with using equation \ref{eq:updateWeigh} on each component of the vector.

\begin{equation}
\label{eq:updateWeigh}
    W_{z}(t+1) = clip(W_{z}(t)+\lfloor\frac{1}{2}(sgn^{+}[e_{z}(t)*2^{-\rho(t)}*U_{z}(t)+1\rfloor,\{\omega_{min},\omega_{max}\})
\end{equation}

Here $e_{z}(t)$, is defined in Equation \ref{eq:avvik} and $\rho(t)$ is defined in equation \ref{eq:rho}:

\begin{equation}
    \label{eq:avvik}
   e_{z}(t) = 2S_{z}(t)-\Tilde{S}_{z}(t)
\end{equation}

\begin{equation}
    \label{eq:rho}
    \rho(t) = clip(v_{min}+\lfloor\frac{t-N_{X}}{t_{inc}}\rfloor, \{v_{min}, v_{max}\})+D-\Omega
\end{equation}

$v_{min}$, $v_{max}$ and $t_{inc}$ are user-defined parameters, with the constraints $-6\leq v_{min}\leq v_{max} \leq 9$ and $2^{4} \leq t_{inc} \leq 2^{11}$, where $t_{inc}$ is a power of two.

\subsubsection*{Prediction values}
The product of the weight vector and local difference vector gives the central local difference $\hat{d}_{z}(t)$. Here R is a user-defined parameter; $max\{32, D+\Omega+2\}\leq R \leq 64$. The last thing that is needed before the output of the predictor can be calculated is the scaled predicted sample value, as shown \ref{eq:scaledpredict}.

\begin{equation}
 \Tilde{S}_{z}(t) = \begin{cases} clip(\lfloor
\frac{\mod_{R}^{*}[\hat{d}_{z}(t)+2^{\Omega}(\sigma_{z}(t-4S_{mid})]}{2^{\Omega+1}} \rfloor+2S_{mid}+1,\{2S_{min},
2S_{max}+1\}) & t>0 \\
2S_{S-1}(t) & t=0, P>0, z>0 \\
2S_{mid} & t=0, P=0, z=0 \end{cases}
\label{eq:scaledpredict}
\end{equation}

The predicted sample value is half of the scaled predicted sample value, this is used to calculate the difference between the predicted sample value and the actual value. The difference values is used as a input for a algorithm that calculates the mapped prediction residuals $\delta_{z}(t)$, which is the output from the predictor stage.
\subsubsection*{Encoder}

With the sample adaptive entropy encoding, the mapped prediction residuals are assigned a variable length binary code word. The code word is based on adaptive code selection statistics, consisting of a accumulator $\Sigma_{z}(t)$ and a counter $\Gamma_{z}(t)$.

The initial value of $\Sigma_{z}(1)$ is defined in \ref{eq:initiacc}.

\begin{equation}
\label{eq:initiacc}
\Sigma_{z}(1) = \lfloor \frac{1}{2^{7}}(3\times2^{\Grave{k}_{z}+6}-49)\Gamma_{z}(t)\rfloor
\end{equation}

The initial value of the counter is $\Gamma_{z}(t) = 2^{\gamma_{0}}$, where $\gamma_{0}$ is a user-defined parameter between $1\leq\gamma_{0}\leq8$. The rest of the values of the accumulator and counter is defined by algorithms using the output from the predictor as a input.

The codeword for the first sample in every band is the same value as the mapped prediction residual, all other samples in each band is found with the parameters $k_{z}(t)$ and $u_{z}(t)$. $k_{z}(t) = 0 $ if $2\Gamma(t)>\Sigma_{z}(t)+\lfloor\frac{49}{2^{7}\Gamma(t)}\rfloor$, and in all other cases $k_{z}(t)$ is the largest positive integer $k_{z}(t) \leq D-2$, when equation \ref{eq:forf} is fulfilled.

\begin{equation}
    \label{eq:forf}
    \Gamma(t)2^{k_{z}(t)} \leq \Sigma_{z}(t)+\lfloor\frac{49}{2^{7}\Gamma(t)}\rfloor
\end{equation}

% \begin{equation}
%     \label{eq:codeword}
%     k_{z}(t) = \begin{cases}
%         0 & 2\Gamma(t)>\Sigma_{z}(t)+\lfloor\frac{49}{2^{7}\Gamma(t)}\rfloor\\
%         max\{1\leq i \leq D-2 | \Gamma(t)2^{i}\leq \Sigma_{z}(t)+ \lfloor\frac{49}{2^{7}\Gamma(t)}\rfloor & otherwise
%     \end{cases}
% \end{equation}

Where $u_{z}(t)$ is

\begin{equation}
    \label{eq:uz}
    u_{z}(t) = \lfloor \frac{\delta(t)}{2^{k_{z}(t)}}\rfloor
\end{equation}

For samples where $u_{z}(t)<U_{max}$ the codeword consists of $u_{z}(t)$ zeros, followed by a one, followed by, followed by the $k_{z}(t)$ least significant bits of $\delta_{z}(t)$.

For samples where $u_{z}(t)\geq U_{max}$, the codeword consists of $U_{max}$ zeros, followed by the D-bit representation of $\delta_{z}(t)$

\subsubsection*{Full and reduced prediction}
It is possible to use the algorithm in reduced mode, when this is used directional local differences is not used during calculations.

\subsubsection*{Implementation}
Figure \ref{fig::compressionpipeline} shows how one pipeline of a multiple pipeline implementation of the CCSDS-123 algorithm looks like. The thick gray lines show the data path through the design, the first four blocks here implements the predictor stage and the last block implements the encoding stage. First in the predictor stage, the local differences are calculated, then multiplied with the weight vector and sent to the predictor block which calculates the predicted values. Then the mapped prediction residuals gets calculated, which is outputed to the encoder stage.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/compressionpipeline.png}
	\caption{Compression implementation CCSDS-123\cite{johanmaster}}
	\label{fig::compressionpipeline}
\end{figure}

Multiple pipelined means that more samples can be processed in parallell, Figure \ref{fig::multiplepipelines} shows how a four lane pipelined implementation looks like. The pixels first arrive at the sample delay block, here they get delayed until all the samples for one neighborhood has arrived, the pixels are expected to come in BIP ordering. Since central local differences is the same for each pipeline, they are stored in a storage that can be accessed by the other pipelines. The same goes for weight and accumulator values. The packer packs the data from the pipelined into specified length words and outputs them.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{images/multiplepipelines.png}
	\caption{Multiple compression lanes CCSDS-123\cite{johanmaster}}
	\label{fig::multiplepipelines}
\end{figure}

The design can be made to process samples on the fly, this means that the samples gets streamed on the fly to the compressor, without any logic to control when input data is on the input. On the fly can be used when the data rate is know, or there is no way to synchronize this, for example if it is connected directly to a camera sensor.

Table \ref{tab::ccsds} shows all the parameters that need to be set for the CCSDS-123 implementation. P is the number of previous bands to use in the difference vector. $t_{inc}$, $v_{max}$ and $v_{min}$ are the values shown in equation \ref{eq:rho} used for updating weight vectors. $\Omega$ is the weight resolution that is used in equation \ref{eq:initweight}, as a initial weight value, and the R constant is used to define the register size in equation \ref{eq:scaledpredict}.  $\gamma_{0}$ is the constant passed to the calculation of the initial value of the encoder counter and $\gamma_{*}$ is a constant that sets the counter size.  $U_{max}$ helps set the length of the codewords.

\begin{table}[]
\centering
\caption{CCSDS-123 implementation parameters}
\label{tab::ccsds}

\begin{tabular}{|l|l|}
\hline
\rowcolor{Lightgray}\textsf{Parameter}& \textsf{Description}                                          \\ \hline
Isunsigned            & Specify if the input is unsigned or signed variables \\ \hline
Little endian         & Specify if the output is little endian or big endian  \\ \hline
On the fly            & Input synchronization                                \\ \hline
Pipelines             & Number of pipelines                                  \\ \hline
Bus width             & The width of the output words                        \\ \hline
Nx                    & Number of pixels in x direction                      \\ \hline
Ny                    & Number of pixels in y direction                      \\ \hline
Nz                    & Number of bands                                      \\ \hline
D                     & Pixel depth                                          \\ \hline
Col oriented          & Column oriented or neighbor oriented                 \\ \hline
P                     & Number of previous bands                             \\ \hline
$t_{inc}$                  & Weight update scaling exponent change interval \\ \hline
$v_{max}$                  & Weight update scaling exponent final           \\ \hline
$v_{min}$                  & Weight update scaling exponent initial         \\ \hline
$\Omega$ & Weight resolution                                    \\ \hline
R                     & Register size                                        \\ \hline
$U_{max}$                  & Unary length limit                                   \\ \hline
$\Grave{k}_{z}$    & Accumulator initialization constant                  \\ \hline
$\gamma_{0}$                    & Initial count exponent                               \\ \hline
$\gamma_{*}$                    & Rescaling counter size                               \\ \hline
\end{tabular}
\end{table}

\FloatBarrier

\section{Binning}
To improve signal to noise ratio and reduce the image size, binning can be used. This technique takes a set of values, combines them together into a value that is representative of the whole set. For the monochrome sensor, pixels that are in the same row in the spectral direction represent different bands of the same area in spatial direction \cite{starrywonders}\cite{microscopyu}. As previously mentioned two binning methods were tested in the specialization project, median and mean binning. Mean binning finds the average value of a set while median binning finds the middle value of the sorted set. Mean binning is the better method to find the central value of a set, but median is more rigid against noise\cite{clinfo}\cite{stattrek}.

A good enough method to do mean binning was found in the specialization project, but not for median binning. The different tested sorting algorithms were random  selection,  insertion  sort,
quick sort, heap sort and counting sort. These were chosen because they are different types of algorithms based on different principles, more information on them can be found in the book Introduction To Algorithms\cite{cormen}. However, one type of algorithm that were not tested, was a parallel algorithm.

A parallel sorting algorithm often used, is bitonic merge sort. It has a running time of  $O(n\log ^{2}(n))$ and is a sorting network\cite{sort1}\cite{sort2}. A sorting network is a sorting algorithm where the comparisons is not data dependent, but rather comparisons are made in a predefined sequence. This predefined sequence for 16 inputs is shown in Figure \ref{fig::bitonicmergenetwork}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/bitonicMerge.PNG}
	\caption{16-input bitonic sorting network\cite{bitonicfig}}
	\label{fig::bitonicmergenetwork}
\end{figure}

The input to a bitonic merge network is a bitonic sequence, this means that one half of a sequence must be in increasing order and the other half in decreasing order. The network then compares corresponding elements of the two halves with each other and moves elements according to the outcome of the comparison. In figure \ref{fig::bitonicmergenetwork}, the arrow points to where the higher value of a comparison moves to. This network consists of four stages, where stage one sorts 2x8 elements, stage two sorts 4x4 elements, stage three sorts 8x2 elements and stage four sorts 18x1 elements. The arrow directions points so that each input to each stage is a bitonic sequence.

\section{Endianness}
Some hardware store numbers from left to right and some hardware store numbers from right to left. This is what is called endianess and can cause problems if different hardware components with different endianness share and operates on the same data. The two different methods of storing data is called little endian and big endian. In big endian numbers the most significant byte is stored first for a number containing more than one byte and in little endian numbers the least significant byte is stored first. Figure \ref{fig::endianess} shows a example of how a unsigned integer with the value 120 is stored for a big endian system and a little endian system.

\begin{figure}[h!]
\centering
\begin{tikzpicture}

\draw[step=0.5cm,color=gray] (0,0) rectangle (1,1);
\draw[step=0.5cm,color=gray] (1,0) rectangle (2,1);
\draw[step=0.5cm,color=gray] (2,0) rectangle (3,1);
\draw[step=0.5cm,color=gray] (3,0) rectangle (4,1);
\node at (0.5,+.5) {120};
\node at (1.5,+.5) {0};
\node at (2.5,+.5) {0};
\node at (3.5,+.5) {0};

\draw[step=0.5cm,color=gray] (0,1) rectangle (1,2);
\draw[step=0.5cm,color=gray] (1,1) rectangle (2,2);
\draw[step=0.5cm,color=gray] (2,1) rectangle (3,2);
\draw[step=0.5cm,color=gray] (3,1) rectangle (4,2);
\node at (0.5,+1.5) {0};
\node at (1.5,+1.5) {0};
\node at (2.5,+1.5) {0};
\node at (3.5,+1.5) {120};

\node at (-0.5,+1.5) {Big};
\node at (-0.5,+.5) {Little};

\node at (0.5,-.5) {0x0};
\node at (1.5,-.5) {0x1};
\node at (2.5,-.5) {0x2};
\node at (3.5,-.5) {0x3};


\end{tikzpicture}

\captionof{figure}{Endianness example}
\label{fig::endianess}
\end{figure}

\section{Tools}
\subsubsection*{Vivado}
Vivado is a tool that can synthesize and implement low level languages such as VHDL and Verilog, as well as high level languages such as C and C++, for a wide variation of different chips\cite{vivado}. It has a IP integrator tool that can be used to import and combine IP (Intellectual property) blocks in a block design. An IP is a hardware design that serves some kind of purpose , that have been packaged into a block, or module. Vivado can package and export designs as IP's and Xilinx have libraries of IPs that can be imported to the design. It has tools for running testbenches on designs and debugging on the target hardware.

\subsubsection*{Petalinux}
To build the OS, petalinux tools is used, this tool builds Embedded Linux systems that is targeted for Xilinx SoCs. Petalinux tools has debug agents, GCC tools and a integrated QEMU  simulator, that simulates the HW, so that the system can be tested. It is possible to customize the boot loader, kernel, file system, libraries and system parameters.

\subsubsection*{Compiler}
The GCC(GNU Compiler Collection) compiler have been used as a compiler in this project. It compiles C and C++ code and is often used with embedded systems. It can compile for many different architecuter, among those are arm chips. It is possible to include directives as input or also known as pragmas, in the code. Directives tells the compiler how to handle a specific section of code, or it can also give rules that apply globally. It is possible to specifiy if the compiler should optimize code and how much optimization should be applied\cite{mangcc}.


\chapter{Testing of camera}
\label{chap::tests}
Testing of the limitations of the camera is shown in this chapter. This works as a continuation of the specialization project. The Zedboard was used for testing in the specialization project, to have consistency this is being used for testing of the new camera in this project as well. A block diagram of the test setup in the processing system is shown on Figure \ref{fig::design1}. A OS is built with petalinux tools and the ueye driver is installed on it the same way as in the specialization project.
\begin{figure}
\vspace{-10cm}

\begin{tikzpicture}

\node [rectangle, fill = white, draw=black, minimum height = 8cm, text width = 8cm, rounded corners] (sw) at (6,0) {};
\node[] at (6,3.7) {Embedded Linux};
\node[] at (0,3.5) {Trigger signal};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (camera) at (0,0) {Camera};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (driver) at (4,0) {Driver};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (program) at (8,0) {C++ program};
\node [rectangle, fill = hue, draw=black, minimum height = .2cm, text width = 2cm, rounded corners, text centered, rotate=90] (interface) at (7,0) {C-interface};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (mem) at (12,0) {Memory};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (storage) at (12,-3) {Storage};
\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (fpga) at (12,3) {FPGA};

\path [draw, ->, thick, -latex] (0,3) -- (camera) ;
\path [draw, ->, thick, -latex] (camera) -- node [above] {GigE} (driver);
\path [draw, ->, thick, -latex] (driver) to[out=45,in=135] (mem);
\path [draw, ->, thick, -latex] (driver) -- (camera);
\path [draw, ->, thick, -latex] (driver) -- (program);
\path [draw, ->, thick, -latex] (program) -- (driver);
\path [draw, ->, thick, -latex] (program) -- (mem);
\path [draw, ->, thick, -latex] (mem) -- (program);
\path [draw, ->, thick, -latex] (program) to[out=270,in=180] (storage);
\path [draw, ->, thick, -latex] (mem) -- (fpga);
%\path [draw, ->, -latex, dashed] (tekst) -- (6.4, -1.1);
\end{tikzpicture}
\caption{System overview testing}
\label{fig::design1}
\end{figure}

\FloatBarrier


\newpage
\begin{wrapfigure}[21]{r}{5cm}
\centering
\begin{tikzpicture}

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (Init) at (0,0) {Initialize};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (start) at (0,-2.5) {Start image capturing};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (pointer) at (0,-5) {Get pointer to memory};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (Bin) at (0,-7.5) {Binning};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (free) at (0,-10) {Free memory};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (cube) at (0,-12.5) {Create cube};

\node [ellipse, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (store) at (0,-15) {Store cube};


\path [draw, ->, thick, -latex] (Init) -- (start);
\path [draw, ->, thick, -latex] (start) -- (pointer);
\path [draw, ->, thick, -latex] (pointer) -- (Bin);
\path [draw, ->, thick, -latex] (Bin) -- (free);
\path [draw, ->, thick, -latex] (free) -- node [right] {No more images} (cube);
\path [draw, ->, thick, -latex] (cube) -- (store);
\path [draw, ->, thick, -latex] (free)  to[out=0,in=0] node [right] {More images} (pointer);

%\path [draw, ->, -latex, dashed] (tekst) -- (6.4, -1.1);
\end{tikzpicture}
\caption{Image aqusition flow testing}
\label{fig::design2}
\end{wrapfigure}

The same C++ testprogram as in the specialization project is being used, the program flow for this is shown on figure \ref{fig::design2}. The first thing it does, is initialize the camera and driver with parameters, such as fps, bit-depth, resolution, exposure time and allocating memory. Then a signal is sent to the driver so that image acquisition can start.

The camera captures images continuously, the driver stores the images in memory areas allocated for it in the initializing phase. Pointers to the next image are fetched from the driver and the image is binned, then moved to a different area in memory. After the image is binned and moved, the memory pointed to must be freed so the driver can use it again. This loops until all the frames are captured, then either a BIL, BIP or BSQ cube is created and stored. When compiling the program, optimization level 3 is used, this is the highest optimization level possible in GCC.

\newpage
\section{Binning}
\label{chap::binningtest}
The highest binning factor requested by the HYPSO mission to test is 12, to solve this 16 input bitonic network is used, for this to work, zero padding can be used, this wont affect the execution time since this algorithm has a predefined sequence. The 12-bit values are 16-bit zero padded values, these values are assumed to be zero padded in the ueye camera. The sorting network gets implemented with NEON SIMD instructions, with 16-bit lanes.

The first and second stage from figure \ref{fig::bitonicmergenetwork} are done with four vectors with four lanes, as shown on Figure \ref{fig:fourlanes}. These are sorted vertically lanewise, so that they are in ascending order downward, V0 does not have to be sorted, since this is the register used for zero padding, there are only zeros in this register. The registers then get transposed, so that each vector have their lanes sorted in ascending order. To do the transposing, two four lane vectors is concatenated into a eight lane vector, then these are zipped two times, this is shown on figure \ref{fig::shuffle1}

\begin{figure}[h!]
\centering
\begin{tikzpicture}

\draw[step=0.5cm,color=gray] (-1,.75) rectangle (-.5,1.25);
\draw[step=0.5cm,color=gray] (-.5,.75) rectangle (0,1.25);
\draw[step=0.5cm,color=gray] (0,.75) rectangle (.5,1.25);
\draw[step=0.5cm,color=gray] (.5,.75) rectangle (1,1.25);
\node at (-0.75,+1) {0};
\node at (-0.25,+1) {0};
\node at (+0.25,+1) {0};
\node at (+0.75,+1) {0};

\draw[step=0.5cm,color=gray] (-1,0) rectangle (-.5,.5);
\draw[step=0.5cm,color=gray] (-.5,.0) rectangle (0,.5);
\draw[step=0.5cm,color=gray] (0,.0) rectangle (.5,.5);
\draw[step=0.5cm,color=gray] (.5,.0) rectangle (1,.5);
\node at (-0.75,+0.25) {i0};
\node at (-0.25,+0.25) {i1};
\node at (+0.25,+0.25) {i2};
\node at (+0.75,+0.25) {i3};

\draw[step=0.5cm,color=gray] (-1,-.5) rectangle (-.5,0);
\draw[step=0.5cm,color=gray] (-.5,-.5) rectangle (0,0);
\draw[step=0.5cm,color=gray] (0,-.5) rectangle (.5,0);
\draw[step=0.5cm,color=gray] (.5,-.5) rectangle (1,0);
\node at (-0.75,-0.25) {j0};
\node at (-0.25,-0.25) {j1};
\node at (+0.25,-0.25) {j2};
\node at (+0.75,-0.25) {j3};

\draw[step=0.5cm,color=gray] (-1,-1) rectangle (-.5,-.5);
\draw[step=0.5cm,color=gray] (-.5,-1) rectangle (0,-.5);
\draw[step=0.5cm,color=gray] (0,-1) rectangle (.5,-.5);
\draw[step=0.5cm,color=gray] (.5,-1) rectangle (1,-.5);
\node at (-0.75,-0.75) {k0};
\node at (-0.25,-0.75) {k1};
\node at (+0.25,-0.75) {k2};
\node at (+0.75,-0.75) {k3};

\node at (-1.3,1) {V0};
\node at (-1.3,.25) {V1};
\node at (-1.3,-.25) {V2};
\node at (-1.3,-.75) {V3};

\draw[red] (.75,-.25) ellipse (.3cm and 1cm);


%%%%LINE
\path [draw, ->, thick, -latex] (1.25,0) -- node [below] {Transpose} (3.7,0);

%%%%Matrix right


\draw[step=0.5cm,color=gray] (4.25,.5) rectangle (4.75,1);
\draw[step=0.5cm,color=gray] (4.75,.5) rectangle (5.25,1);
\draw[step=0.5cm,color=gray] (5.25,.5) rectangle (5.75,1);
\draw[step=0.5cm,color=gray] (5.75,.5) rectangle (6.25,1);
\node at (4.5,+.75) {0};
\node at (5,+.75) {i0};
\node at (5.5,+.75) {j0};
\node at (6,+.75) {k0};

\draw[step=0.5cm,color=gray] (4.25,0) rectangle (4.75,.5);
\draw[step=0.5cm,color=gray] (4.75,.0) rectangle (5.25,.5);
\draw[step=0.5cm,color=gray] (5.25,.0) rectangle (5.75,.5);
\draw[step=0.5cm,color=gray] (5.75,.0) rectangle (6.25,.5);
\node at (4.5,+0.25) {0};
\node at (5,+0.25) {i1};
\node at (5.5,+0.25) {j1};
\node at (6,+0.25) {k1};

\draw[step=0.5cm,color=gray] (4.25,-.5) rectangle (4.75,0);
\draw[step=0.5cm,color=gray] (4.75,-.5) rectangle (5.25,0);
\draw[step=0.5cm,color=gray] (5.25,-.5) rectangle (5.75,0);
\draw[step=0.5cm,color=gray] (5.75,-.5) rectangle (6.25,0);
\node at (4.5,-0.25) {0};
\node at (5,-0.25) {i2};
\node at (5.5,-0.25) {j2};
\node at (6,-0.25) {k2};

\draw[step=0.5cm,color=gray] (4.25,-1) rectangle (4.75,-.5);
\draw[step=0.5cm,color=gray] (4.75,-1) rectangle (5.25,-.5);
\draw[step=0.5cm,color=gray] (5.25,-1) rectangle (5.75,-.5);
\draw[step=0.5cm,color=gray] (5.75,-1) rectangle (6.25,-.5);
\node at (4.5,-0.75) {0};
\node at (5,-0.75) {i3};
\node at (5.5,-0.75) {j3};
\node at (6,-0.75) {k3};

\node at (4,.75) {V0};
\node at (4,.25) {V1};
\node at (4,-.25) {V2};
\node at (4,-.75) {V3};

\end{tikzpicture}

\captionof{figure}{Stage 1 and 2 merging network}
\label{fig:fourlanes}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tikzpicture}

\draw[step=0.5cm,color=gray] (0,1.5) rectangle (.5,2);
\draw[step=0.5cm,color=gray] (.5,1.5) rectangle (1,2);
\draw[step=0.5cm,color=gray] (1,1.5) rectangle (1.5,2);
\draw[step=0.5cm,color=gray] (1.5,1.5) rectangle (2,2);
\draw[step=0.5cm,color=gray] (2,1.5) rectangle (2.5,2);
\draw[step=0.5cm,color=gray] (2.5,1.5) rectangle (3,2);
\draw[step=0.5cm,color=gray] (3,1.5) rectangle (3.5,2);
\draw[step=0.5cm,color=gray] (3.5,1.5) rectangle (4,2);

\draw[step=0.5cm,color=gray] (0,0) rectangle (.5,.5);
\draw[step=0.5cm,color=gray] (.5,0) rectangle (1,.5);
\draw[step=0.5cm,color=gray] (1,0) rectangle (1.5,.5);
\draw[step=0.5cm,color=gray] (1.5,0) rectangle (2,.5);
\draw[step=0.5cm,color=gray] (2,0) rectangle (2.5,.5);
\draw[step=0.5cm,color=gray] (2.5,0) rectangle (3,.5);
\draw[step=0.5cm,color=gray] (3,0) rectangle (3.5,.5);
\draw[step=0.5cm,color=gray] (3.5,0) rectangle (4,.5);

\draw[step=0.5cm,color=gray] (3.75,.75) rectangle (5,1.25);

\draw[->] (4,1.75) to[out=0,in=90] (4.25,1.25);
\draw[->] (4.5,1.25) to[out=90,in=180] (4.75,1.75);

\draw[->] (4,.25) to[out=0,in=-90] (4.25,.75);
\draw[->] (4.5,.75) to[out=-90,in=180] (4.75,.25);

\draw[step=0.5cm,color=gray] (0+4.75,1.5) rectangle (.5+4.75,2);
\draw[step=0.5cm,color=gray] (.5+4.75,1.5) rectangle (1+4.75,2);
\draw[step=0.5cm,color=gray] (1+4.75,1.5) rectangle (1.5+4.75,2);
\draw[step=0.5cm,color=gray] (1.5+4.75,1.5) rectangle (2+4.75,2);
\draw[step=0.5cm,color=gray] (2+4.75,1.5) rectangle (2.5+4.75,2);
\draw[step=0.5cm,color=gray] (2.5+4.75,1.5) rectangle (3+4.75,2);
\draw[step=0.5cm,color=gray] (3+4.75,1.5) rectangle (3.5+4.75,2);
\draw[step=0.5cm,color=gray] (3.5+4.75,1.5) rectangle (4+4.75,2);

\draw[step=0.5cm,color=gray] (0+4.75,0) rectangle (.5+4.75,.5);
\draw[step=0.5cm,color=gray] (.5+4.75,0) rectangle (1+4.75,.5);
\draw[step=0.5cm,color=gray] (1+4.75,0) rectangle (1.5+4.75,.5);
\draw[step=0.5cm,color=gray] (1.5+4.75,0) rectangle (2+4.75,.5);
\draw[step=0.5cm,color=gray] (2+4.75,0) rectangle (2.5+4.75,.5);
\draw[step=0.5cm,color=gray] (2.5+4.75,0) rectangle (3+4.75,.5);
\draw[step=0.5cm,color=gray] (3+4.75,0) rectangle (3.5+4.75,.5);
\draw[step=0.5cm,color=gray] (3.5+4.75,0) rectangle (4+4.75,.5);

\draw[step=0.5cm,color=gray] (3.75+4.75,.75) rectangle (5+4.75,1.25);

\draw[->] (4+4.75,1.75) to[out=0,in=90] (4.25+4.75,1.25);
\draw[->] (4.5+4.75,1.25) to[out=90,in=180] (4.75+4.75,1.75);

\draw[->] (4+4.75,.25) to[out=0,in=-90] (4.25+4.75,.75);
\draw[->] (4.5+4.75,.75) to[out=-90,in=180] (4.75+4.75,.25);

\draw[step=0.5cm,color=gray] (0+4.75+4.75,1.5) rectangle (.5+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (.5+4.75+4.75,1.5) rectangle (1+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (1+4.75+4.75,1.5) rectangle (1.5+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (1.5+4.75+4.75,1.5) rectangle (2+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (2+4.75+4.75,1.5) rectangle (2.5+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (2.5+4.75+4.75,1.5) rectangle (3+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (3+4.75+4.75,1.5) rectangle (3.5+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (3.5+4.75+4.75,1.5) rectangle (4+4.75+4.75,2);

\draw[step=0.5cm,color=gray] (0+4.75+4.75,0) rectangle (.5+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (.5+4.75+4.75,0) rectangle (1+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (1+4.75+4.75,0) rectangle (1.5+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (1.5+4.75+4.75,0) rectangle (2+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (2+4.75+4.75,0) rectangle (2.5+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (2.5+4.75+4.75,0) rectangle (3+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (3+4.75+4.75,0) rectangle (3.5+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (3.5+4.75+4.75,0) rectangle (4+4.75+4.75,.5);

\node at (+0.25,+1.75) {\footnotesize{0}};
\node at (+0.75,+1.75) {\footnotesize{0}};
\node at (1.25,+1.75) {\footnotesize{0}};
\node at (1.75,+1.75) {\footnotesize{0}};
\node at (2.25,+1.75) {\footnotesize{I0}};
\node at (2.75,+1.75) {\footnotesize{I1}};
\node at (3.25,+1.75) {\footnotesize{I2}};
\node at (3.75,+1.75) {\footnotesize{I3}};

\node at (+0.25,+.25) {\footnotesize{J0}};
\node at (+0.75,.25) {\footnotesize{J1}};
\node at (1.25,.25) {\footnotesize{J2}};
\node at (1.75,.25) {\footnotesize{J3}};
\node at (2.25,.25) {\footnotesize{K0}};
\node at (2.75,.25) {\footnotesize{K1}};
\node at (3.25,.25) {\footnotesize{K2}};
\node at (3.75,.25) {\footnotesize{K3}};


\node at (+0.25+4.75,+1.75) {\footnotesize{0}};
\node at (+0.75+4.75,+1.75) {\footnotesize{J0}};
\node at (1.25+4.75,+1.75) {\footnotesize{0}};
\node at (1.75+4.75,+1.75) {\footnotesize{J1}};
\node at (2.25+4.75,+1.75) {\footnotesize{0}};
\node at (2.75+4.75,+1.75) {\footnotesize{J2}};
\node at (3.25+4.75,+1.75) {\footnotesize{0}};
\node at (3.75+4.75,+1.75) {\footnotesize{J3}};

\node at (+0.25+4.75,+.25) {\footnotesize{I0}};
\node at (+0.75+4.75,.25) {\footnotesize{K0}};
\node at (1.25+4.75,.25) {\footnotesize{I1}};
\node at (1.75+4.75,.25) {\footnotesize{K1}};
\node at (2.25+4.75,.25) {\footnotesize{I2}};
\node at (2.75+4.75,.25) {\footnotesize{K2}};
\node at (3.25+4.75,.25) {\footnotesize{I3}};
\node at (3.75+4.75,.25) {\footnotesize{K3}};


\node at (+0.25+4.75+4.75,+1.75) {\footnotesize{0}};
\node at (+0.75+4.75+4.75,+1.75) {\footnotesize{I0}};
\node at (1.25+4.75+4.75,+1.75) {\footnotesize{J0}};
\node at (1.75+4.75+4.75,+1.75) {\footnotesize{K0}};
\node at (2.25+4.75+4.75,+1.75) {\footnotesize{0}};
\node at (2.75+4.75+4.75,+1.75) {\footnotesize{I1}};
\node at (3.25+4.75+4.75,+1.75) {\footnotesize{J1}};
\node at (3.75+4.75+4.75,+1.75) {\footnotesize{K1}};

\node at (+0.25+4.75+4.75,+.25) {\footnotesize{0}};
\node at (+0.75+4.75+4.75,.25) {\footnotesize{I2}};
\node at (1.25+4.75+4.75,.25) {\footnotesize{J2}};
\node at (1.75+4.75+4.75,.25) {\footnotesize{K2}};
\node at (2.25+4.75+4.75,.25) {\footnotesize{0}};
\node at (2.75+4.75+4.75,.25) {\footnotesize{I3}};
\node at (3.25+4.75+4.75,.25) {\footnotesize{J3}};
\node at (3.75+4.75+4.75,.25) {\footnotesize{K3}};

\node at (4.4,1) {\footnotesize{Zip}};
\node at (4.4+4.75,1) {\footnotesize{Zip}};

\end{tikzpicture}

\captionof{figure}{Two zip instructions NEON}
\label{fig::shuffle1}
\end{figure}


Now one of the eight lane vectors gets reversed with two four lane sub elements, these two eight lane vectors are the inputs for stage three. This stage of the network sorts 8x2 elements in opposite directions, each half of the eight lane vectors are each of these 8x2 elements. The vectors is compared and a max and a min vector is produced, as shown in figure \ref{fig:stage3input}. Instructions to shuffle the max and min values to the right lane and vectors is done for the next comparison. This is repeated another time, then the last comparison is done, this produces a high and a low vector which is zipped.

\begin{figure}[h!]
\centering
\begin{tikzpicture}

\filldraw[fill=YellowGreen] (-1,.75) rectangle (1,1.25);
\filldraw[fill=ProcessBlue] (1,.75) rectangle (3,1.25);

\draw[step=0.5cm,color=gray] (-1,.75) rectangle (-.5,1.25);
\draw[step=0.5cm,color=gray] (-.5,.75) rectangle (0,1.25);
\draw[step=0.5cm,color=gray] (0,.75) rectangle (.5,1.25);
\draw[step=0.5cm,color=gray] (.5,.75) rectangle (1,1.25);

\draw[step=0.5cm,color=gray] (1,.75) rectangle (1.5,1.25);
\draw[step=0.5cm,color=gray] (1.5,.75) rectangle (2,1.25);
\draw[step=0.5cm,color=gray] (2,.75) rectangle (2.5,1.25);
\draw[step=0.5cm,color=gray] (2.5,.75) rectangle (3,1.25);
\node at (-0.75,+1) {\footnotesize{k0}};
\node at (-0.25,+1) {\footnotesize{j0}};
\node at (+0.25,+1) {\footnotesize{i0}};
\node at (+0.75,+1) {\footnotesize{0}};
\node at (1.25,+1) {\footnotesize{k1}};
\node at (1.75,+1) {\footnotesize{j1}};
\node at (2.25,+1) {\footnotesize{i1}};
\node at (2.75,+1) {\footnotesize{0}};


\filldraw[fill=YellowGreen] (-1,0) rectangle (1,.5);
\filldraw[fill=ProcessBlue] (1,.0) rectangle (3,.5);

\draw[step=0.5cm,color=gray] (-1,0) rectangle (-.5,.5);
\draw[step=0.5cm,color=gray] (-.5,.0) rectangle (0,.5);
\draw[step=0.5cm,color=gray] (0,.0) rectangle (.5,.5);
\draw[step=0.5cm,color=gray] (.5,.0) rectangle (1,.5);

\draw[step=0.5cm,color=gray] (1,0) rectangle (1.5,.5);
\draw[step=0.5cm,color=gray] (1.5,.0) rectangle (2,.5);
\draw[step=0.5cm,color=gray] (2,.0) rectangle (2.5,.5);
\draw[step=0.5cm,color=gray] (2.5,.0) rectangle (3,.5);
\node at (-0.75,+0.25) {\footnotesize{0}};
\node at (-0.25,+0.25) {\footnotesize{i2}};
\node at (+0.25,+0.25) {\footnotesize{j2}};
\node at (+0.75,+0.25) {\footnotesize{k2}};
\node at (1.25,+0.25) {\footnotesize{0}};
\node at (1.75,+0.25) {\footnotesize{i3}};
\node at (2.25,+0.25) {\footnotesize{j3}};
\node at (2.75,+0.25) {\footnotesize{k3}};


\filldraw[fill=YellowGreen] (-1,-.75) rectangle (1,.-.25);
\filldraw[fill=ProcessBlue] (1,-.75) rectangle (3,-.25);

\draw[step=0.5cm,color=gray] (-1,-.75) rectangle (-.5,-.25);
\draw[step=0.5cm,color=gray] (-.5,-.75) rectangle (0,-.25);
\draw[step=0.5cm,color=gray] (0,-.75) rectangle (.5,-.25);
\draw[step=0.5cm,color=gray] (.5,-.75) rectangle (1,-.25);

\draw[step=0.5cm,color=gray] (1,-.75) rectangle (1.5,-.25);
\draw[step=0.5cm,color=gray] (1.5,-.75) rectangle (2,-.25);
\draw[step=0.5cm,color=gray] (2,-.75) rectangle (2.5,-.25);
\draw[step=0.5cm,color=gray] (2.5,-.75) rectangle (3,-.25);
\node at (-0.75,-.5) {\footnotesize{H0}};
\node at (-0.25,-.5) {\footnotesize{H1}};
\node at (+0.25,-.5) {\footnotesize{H2}};
\node at (+0.75,-.5) {\footnotesize{H3}};
\node at (1.25,-.5) {\footnotesize{H4}};
\node at (1.75,-.5) {\footnotesize{H5}};
\node at (2.25,-.5) {\footnotesize{H6}};
\node at (2.75,-.5) {\footnotesize{H7}};


\filldraw[fill=YellowGreen] (-1,-1.5) rectangle (1,-1);
\filldraw[fill=ProcessBlue] (1,-1.5) rectangle (3,-1);

\draw[step=0.5cm,color=gray] (-1,-1.5) rectangle (-.5,-1);
\draw[step=0.5cm,color=gray] (-.5,-1.5) rectangle (0,-1);
\draw[step=0.5cm,color=gray] (0,-1.5) rectangle (.5,-1);
\draw[step=0.5cm,color=gray] (.5,-1.5) rectangle (1,-1);

\draw[step=0.5cm,color=gray] (1,-1.5) rectangle (1.5,-1);
\draw[step=0.5cm,color=gray] (1.5,-1.5) rectangle (2,-1);
\draw[step=0.5cm,color=gray] (2,-1.5) rectangle (2.5,-1);
\draw[step=0.5cm,color=gray] (2.5,-1.5) rectangle (3,-1);
\node at (-0.75,-1.25) {\footnotesize{L0}};
\node at (-0.25,-1.25) {\footnotesize{L1}};
\node at (+0.25,-1.25) {\footnotesize{L2}};
\node at (+0.75,-1.25) {\footnotesize{L3}};
\node at (1.25,-1.25) {\footnotesize{L4}};
\node at (1.75,-1.25) {\footnotesize{L5}};
\node at (2.25,-1.25) {\footnotesize{L6}};
\node at (2.75,-1.25) {\footnotesize{L7}};


\node at (3.25,.6) {$<$};
\node at (3.25,-.2) {$=$};

\node at (-1.3,1) {V0};
\node at (-1.3,.25) {V1};
\node at (-1.5,-.5) {High};
\node at (-1.5,-1.25) {Low};

%%Shuffle
\draw[->] (.75,-1.75) .. controls (.75,-2.2) and (1.25,-1.8) .. (1.25,-2.25);
\draw[->] (1.25,-1.75) .. controls (1.25,-2.2) and (.75,-1.8).. (.75,-2.25);
\node at (2,-2) {Shuffle};
%% Result bottpm  left

\filldraw[fill=YellowGreen] (-1,-3) rectangle (1,-2.5);
\filldraw[fill=ProcessBlue] (1,-3) rectangle (3,-2.5);

\draw[step=0.5cm,color=gray] (-1,-3) rectangle (-.5,-2.5);
\draw[step=0.5cm,color=gray] (-.5,-3) rectangle (0,-2.5);
\draw[step=0.5cm,color=gray] (0,-3) rectangle (.5,-2.5);
\draw[step=0.5cm,color=gray] (.5,-3) rectangle (1,-2.5);

\draw[step=0.5cm,color=gray] (1,-3) rectangle (1.5,-2.5);
\draw[step=0.5cm,color=gray] (1.5,-3) rectangle (2,-2.5);
\draw[step=0.5cm,color=gray] (2,-3) rectangle (2.5,-2.5);
\draw[step=0.5cm,color=gray] (2.5,-3) rectangle (3,-2.5);
\node at (-0.75,-2.75) {\footnotesize{L0}};
\node at (-0.25,-2.75) {\footnotesize{L1}};
\node at (+0.25,-2.75) {\footnotesize{H0}};
\node at (+0.75,-2.75) {\footnotesize{H1}};
\node at (1.25,-2.75) {\footnotesize{L4}};
\node at (1.75,-2.75) {\footnotesize{L5}};
\node at (2.25,-2.75) {\footnotesize{H4}};
\node at (2.75,-2.75) {\footnotesize{H5}};


\filldraw[fill=YellowGreen] (-1,-3.75) rectangle (1,-3.25);
\filldraw[fill=ProcessBlue] (1,-3.75) rectangle (3,-3.25);

\draw[step=0.5cm,color=gray] (-1,-3.75) rectangle (-.5,-3.25);
\draw[step=0.5cm,color=gray] (-.5,-3.75) rectangle (0,-3.25);
\draw[step=0.5cm,color=gray] (0,-3.75) rectangle (.5,-3.25);
\draw[step=0.5cm,color=gray] (.5,-3.75) rectangle (1,-3.25);

\draw[step=0.5cm,color=gray] (1,-3.75) rectangle (1.5,-3.25);
\draw[step=0.5cm,color=gray] (1.5,-3.75) rectangle (2,-3.25);
\draw[step=0.5cm,color=gray] (2,-3.75) rectangle (2.5,-3.25);
\draw[step=0.5cm,color=gray] (2.5,-3.75) rectangle (3,-3.25);
\node at (-0.75,-3.5) {\footnotesize{L2}};
\node at (-0.25,-3.5) {\footnotesize{L3}};
\node at (+0.25,-3.5) {\footnotesize{H2}};
\node at (+0.75,-3.5) {\footnotesize{H3}};
\node at (1.25,-3.5) {\footnotesize{L6}};
\node at (1.75,-3.5) {\footnotesize{L7}};
\node at (2.25,-3.5) {\footnotesize{H6}};
\node at (2.75,-3.5) {\footnotesize{H7}};


\node at (3.25,-3.1) {$<$};

\node at (-1.3,-2.75) {V0};
\node at (-1.3,-3.75) {V1};


\draw[->] (3.5,-3.1) .. controls (4.8,-3.1) and (3.5,.5) .. (4.8,.5);

%% Result top right

\filldraw[fill=YellowGreen] (6,.75) rectangle (8,1.25);
\filldraw[fill=ProcessBlue] (8,.75) rectangle (10,1.25);


\draw[step=0.5cm,color=gray] (6,.75) rectangle (6.5,1.25);
\draw[step=0.5cm,color=gray] (6.5,.75) rectangle (7,1.25);
\draw[step=0.5cm,color=gray] (7,.75) rectangle (7.5,1.25);
\draw[step=0.5cm,color=gray] (7.5,.75) rectangle (8,1.25);

\draw[step=0.5cm,color=gray] (8,.75) rectangle (8.5,1.25);
\draw[step=0.5cm,color=gray] (8.5,.75) rectangle (9,1.25);
\draw[step=0.5cm,color=gray] (9,.75) rectangle (9.5,1.25);
\draw[step=0.5cm,color=gray] (9.5,.75) rectangle (10,1.25);
\node at (6.25,+1) {\footnotesize{H0}};
\node at (6.75,+1) {\footnotesize{H1}};
\node at (7.25,+1) {\footnotesize{H2}};
\node at (7.75,+1) {\footnotesize{H3}};
\node at (8.25,+1) {\footnotesize{H4}};
\node at (8.75,+1) {\footnotesize{H5}};
\node at (9.25,+1) {\footnotesize{H6}};
\node at (9.75,+1) {\footnotesize{H7}};


\filldraw[fill=YellowGreen] (6,0) rectangle (8,.5);
\filldraw[fill=ProcessBlue] (8,0) rectangle (10,.5);

\draw[step=0.5cm,color=gray] (6,0) rectangle (6.5,.5);
\draw[step=0.5cm,color=gray] (6.5,.0) rectangle (7,.5);
\draw[step=0.5cm,color=gray] (7,.0) rectangle (7.5,.5);
\draw[step=0.5cm,color=gray] (7.5,.0) rectangle (8,.5);

\draw[step=0.5cm,color=gray] (8,0) rectangle (8.5,.5);
\draw[step=0.5cm,color=gray] (8.5,.0) rectangle (9,.5);
\draw[step=0.5cm,color=gray] (9,.0) rectangle (9.5,.5);
\draw[step=0.5cm,color=gray] (9.5,.0) rectangle (10,.5);
\node at (6.25,.25) {\footnotesize{L0}};
\node at (6.75,.25) {\footnotesize{L1}};
\node at (7.25,.25) {\footnotesize{L2}};
\node at (7.75,.25) {\footnotesize{L3}};
\node at (8.25,.25) {\footnotesize{L4}};
\node at (8.75,.25) {\footnotesize{L5}};
\node at (9.25,.25) {\footnotesize{L6}};
\node at (9.75,.25) {\footnotesize{L7}};

\node at (10.25,.6) {$<$};

\node at (5.5,1) {High};
\node at (5.5,.25) {Low};

%%Shuffle right
\draw[->] (7.75,-.25) .. controls (7.75,-.7) and (8.25,-.3) .. (8.25,-.75);
\draw[->] (8.25,-.25) .. controls (8.25,-.7) and (7.75,-.3).. (7.75,-.75);
\node at (9,-.5) {Shuffle};
%% Shuffled middle right

\filldraw[fill=YellowGreen] (6,-1.5) rectangle (8,-1);
\filldraw[fill=ProcessBlue] (8,-1.5) rectangle (10,-1);

\draw[step=0.5cm,color=gray] (6,-1.5) rectangle (6.5,-1);
\draw[step=0.5cm,color=gray] (6.5,-1.5) rectangle (7,-1);
\draw[step=0.5cm,color=gray] (7,-1.5) rectangle (7.5,-1);
\draw[step=0.5cm,color=gray] (7.5,-1.5) rectangle (8,-1);

\draw[step=0.5cm,color=gray] (8,-1.5) rectangle (8.5,-1);
\draw[step=0.5cm,color=gray] (8.5,-1.5) rectangle (9,-1);
\draw[step=0.5cm,color=gray] (9,-1.5) rectangle (9.5,-1);
\draw[step=0.5cm,color=gray] (9.5,-1.5) rectangle (10,-1);
\node at (6.25,-1.25) {\footnotesize{L0}};
\node at (6.75,-1.25) {\footnotesize{H0}};
\node at (7.25,-1.25) {\footnotesize{L2}};
\node at (7.75,-1.25) {\footnotesize{H2}};
\node at (8.25,-1.25) {\footnotesize{L4}};
\node at (8.75,-1.25) {\footnotesize{H4}};
\node at (9.25,-1.25) {\footnotesize{L6}};
\node at (9.75,-1.25) {\footnotesize{H6}};


\filldraw[fill=YellowGreen] (6,-2.25) rectangle (8,-1.75);
\filldraw[fill=ProcessBlue] (8,-2.25) rectangle (10,-1.75);

\draw[step=0.5cm,color=gray] (6,-2.25) rectangle (6.5,-1.75);
\draw[step=0.5cm,color=gray] (6.5,-2.25) rectangle (7,-1.75);
\draw[step=0.5cm,color=gray] (7,-2.25) rectangle (7.5,-1.75);
\draw[step=0.5cm,color=gray] (7.5,-2.25) rectangle (8,-1.75);

\draw[step=0.5cm,color=gray] (8,-2.25) rectangle (8.5,-1.75);
\draw[step=0.5cm,color=gray] (8.5,-2.25) rectangle (9,-1.75);
\draw[step=0.5cm,color=gray] (9,-2.25) rectangle (9.5,-1.75);
\draw[step=0.5cm,color=gray] (9.5,-2.25) rectangle (10,-1.75);
\node at (6.25,-2) {\footnotesize{L1}};
\node at (6.75,-2) {\footnotesize{H1}};
\node at (7.25,-2) {\footnotesize{L3}};
\node at (7.75,-2) {\footnotesize{H3}};
\node at (8.25,-2) {\footnotesize{L5}};
\node at (8.75,-2) {\footnotesize{H5}};
\node at (9.25,-2) {\footnotesize{L7}};
\node at (9.75,-2) {\footnotesize{H7}};

\node at (10.25,-1.6) {$<$};

\node at (5.5,-1.25) {V0};
\node at (5.5,-2) {V1};

%%%Result bottom right
\filldraw[fill=YellowGreen] (6,-3) rectangle (8,-2.5);
\filldraw[fill=ProcessBlue] (8,-3) rectangle (10,-2.5);

\draw[step=0.5cm,color=gray] (6,-3) rectangle (6.5,-2.5);
\draw[step=0.5cm,color=gray] (6.5,-3) rectangle (7,-2.5);
\draw[step=0.5cm,color=gray] (7,-3) rectangle (7.5,-2.5);
\draw[step=0.5cm,color=gray] (7.5,-3) rectangle (8,-2.5);

\draw[step=0.5cm,color=gray] (8,-3) rectangle (8.5,-2.5);
\draw[step=0.5cm,color=gray] (8.5,-3) rectangle (9,-2.5);
\draw[step=0.5cm,color=gray] (9,-3) rectangle (9.5,-2.5);
\draw[step=0.5cm,color=gray] (9.5,-3) rectangle (10,-2.5);

\node at (6.25,-2.75) {\footnotesize{H0}};
\node at (6.75,-2.75) {\footnotesize{H1}};
\node at (7.25,-2.75) {\footnotesize{H2}};
\node at (7.75,-2.75) {\footnotesize{H3}};
\node at (8.25,-2.75) {\footnotesize{H4}};
\node at (8.75,-2.75) {\footnotesize{H5}};
\node at (9.25,-2.75) {\footnotesize{H6}};
\node at (9.75,-2.75) {\footnotesize{H7}};



\filldraw[fill=YellowGreen] (6,-3.75) rectangle (8,-3.25);
\filldraw[fill=ProcessBlue] (8,-3.75) rectangle (10,-3.25);

\draw[step=0.5cm,color=gray] (6,-3.75) rectangle (6.5,-3.25);
\draw[step=0.5cm,color=gray] (6.5,-3.75) rectangle (7,-3.25);
\draw[step=0.5cm,color=gray] (7,-3.75) rectangle (7.5,-3.25);
\draw[step=0.5cm,color=gray] (7.5,-3.75) rectangle (8,-3.25);

\draw[step=0.5cm,color=gray] (8,-3.75) rectangle (8.5,-3.25);
\draw[step=0.5cm,color=gray] (8.5,-3.75) rectangle (9,-3.25);
\draw[step=0.5cm,color=gray] (9,-3.75) rectangle (9.5,-3.25);
\draw[step=0.5cm,color=gray] (9.5,-3.75) rectangle (10,-3.25);

\node at (6.25,-3.5) {\footnotesize{L0}};
\node at (6.75,-3.5) {\footnotesize{L1}};
\node at (7.25,-3.5) {\footnotesize{L2}};
\node at (7.75,-3.5) {\footnotesize{L3}};
\node at (8.25,-3.5) {\footnotesize{L4}};
\node at (8.75,-3.5) {\footnotesize{L5}};
\node at (9.25,-3.5) {\footnotesize{L6}};
\node at (9.75,-3.5) {\footnotesize{L7}};

\node at (10.25,-2.4) {=};

\node at (5.5,-2.75) {High};
\node at (5.5,-3.5) {Low};

\end{tikzpicture}

\captionof{figure}{Stage 3 merging network}
\label{fig:stage3input}
\end{figure}

The first shuffle instruction in figure \ref{fig:stage3input} is produced with a transpose and a zip instruction, this is shown on figure \ref{fig:shuffle2}. The second shuffle is produced with one transpose instruction.


\begin{figure}[h!]
\centering
\begin{tikzpicture}

\draw[step=0.5cm,color=gray] (0,1.5) rectangle (.5,2);
\draw[step=0.5cm,color=gray] (.5,1.5) rectangle (1,2);
\draw[step=0.5cm,color=gray] (1,1.5) rectangle (1.5,2);
\draw[step=0.5cm,color=gray] (1.5,1.5) rectangle (2,2);
\draw[step=0.5cm,color=gray] (2,1.5) rectangle (2.5,2);
\draw[step=0.5cm,color=gray] (2.5,1.5) rectangle (3,2);
\draw[step=0.5cm,color=gray] (3,1.5) rectangle (3.5,2);
\draw[step=0.5cm,color=gray] (3.5,1.5) rectangle (4,2);

\draw[step=0.5cm,color=gray] (0,0) rectangle (.5,.5);
\draw[step=0.5cm,color=gray] (.5,0) rectangle (1,.5);
\draw[step=0.5cm,color=gray] (1,0) rectangle (1.5,.5);
\draw[step=0.5cm,color=gray] (1.5,0) rectangle (2,.5);
\draw[step=0.5cm,color=gray] (2,0) rectangle (2.5,.5);
\draw[step=0.5cm,color=gray] (2.5,0) rectangle (3,.5);
\draw[step=0.5cm,color=gray] (3,0) rectangle (3.5,.5);
\draw[step=0.5cm,color=gray] (3.5,0) rectangle (4,.5);

\draw[step=0.5cm,color=gray] (3.75,.75) rectangle (5,1.25);

\draw[->] (4,1.75) to[out=0,in=90] (4.25,1.25);
\draw[->] (4.5,1.25) to[out=90,in=180] (4.75,1.75);

\draw[->] (4,.25) to[out=0,in=-90] (4.25,.75);
\draw[->] (4.5,.75) to[out=-90,in=180] (4.75,.25);

\draw[step=0.5cm,color=gray] (0+4.75,1.5) rectangle (.5+4.75,2);
\draw[step=0.5cm,color=gray] (.5+4.75,1.5) rectangle (1+4.75,2);
\draw[step=0.5cm,color=gray] (1+4.75,1.5) rectangle (1.5+4.75,2);
\draw[step=0.5cm,color=gray] (1.5+4.75,1.5) rectangle (2+4.75,2);
\draw[step=0.5cm,color=gray] (2+4.75,1.5) rectangle (2.5+4.75,2);
\draw[step=0.5cm,color=gray] (2.5+4.75,1.5) rectangle (3+4.75,2);
\draw[step=0.5cm,color=gray] (3+4.75,1.5) rectangle (3.5+4.75,2);
\draw[step=0.5cm,color=gray] (3.5+4.75,1.5) rectangle (4+4.75,2);

\draw[step=0.5cm,color=gray] (0+4.75,0) rectangle (.5+4.75,.5);
\draw[step=0.5cm,color=gray] (.5+4.75,0) rectangle (1+4.75,.5);
\draw[step=0.5cm,color=gray] (1+4.75,0) rectangle (1.5+4.75,.5);
\draw[step=0.5cm,color=gray] (1.5+4.75,0) rectangle (2+4.75,.5);
\draw[step=0.5cm,color=gray] (2+4.75,0) rectangle (2.5+4.75,.5);
\draw[step=0.5cm,color=gray] (2.5+4.75,0) rectangle (3+4.75,.5);
\draw[step=0.5cm,color=gray] (3+4.75,0) rectangle (3.5+4.75,.5);
\draw[step=0.5cm,color=gray] (3.5+4.75,0) rectangle (4+4.75,.5);

\draw[step=0.5cm,color=gray] (3.75+4.75,.75) rectangle (5+4.75,1.25);

\draw[->] (4+4.75,1.75) to[out=0,in=90] (4.25+4.75,1.25);
\draw[->] (4.5+4.75,1.25) to[out=90,in=180] (4.75+4.75,1.75);

\draw[->] (4+4.75,.25) to[out=0,in=-90] (4.25+4.75,.75);
\draw[->] (4.5+4.75,.75) to[out=-90,in=180] (4.75+4.75,.25);

\draw[step=0.5cm,color=gray] (0+4.75+4.75,1.5) rectangle (.5+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (.5+4.75+4.75,1.5) rectangle (1+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (1+4.75+4.75,1.5) rectangle (1.5+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (1.5+4.75+4.75,1.5) rectangle (2+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (2+4.75+4.75,1.5) rectangle (2.5+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (2.5+4.75+4.75,1.5) rectangle (3+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (3+4.75+4.75,1.5) rectangle (3.5+4.75+4.75,2);
\draw[step=0.5cm,color=gray] (3.5+4.75+4.75,1.5) rectangle (4+4.75+4.75,2);

\draw[step=0.5cm,color=gray] (0+4.75+4.75,0) rectangle (.5+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (.5+4.75+4.75,0) rectangle (1+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (1+4.75+4.75,0) rectangle (1.5+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (1.5+4.75+4.75,0) rectangle (2+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (2+4.75+4.75,0) rectangle (2.5+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (2.5+4.75+4.75,0) rectangle (3+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (3+4.75+4.75,0) rectangle (3.5+4.75+4.75,.5);
\draw[step=0.5cm,color=gray] (3.5+4.75+4.75,0) rectangle (4+4.75+4.75,.5);

\node at (+0.25,+1.75) {\footnotesize{H0}};
\node at (+0.75,+1.75) {\footnotesize{H1}};
\node at (1.25,+1.75) {\footnotesize{H2}};
\node at (1.75,+1.75) {\footnotesize{H3}};
\node at (2.25,+1.75) {\footnotesize{H4}};
\node at (2.75,+1.75) {\footnotesize{H5}};
\node at (3.25,+1.75) {\footnotesize{H6}};
\node at (3.75,+1.75) {\footnotesize{H7}};

\node at (+0.25,+.25) {\footnotesize{L0}};
\node at (+0.75,.25) {\footnotesize{L1}};
\node at (1.25,.25) {\footnotesize{L2}};
\node at (1.75,.25) {\footnotesize{L3}};
\node at (2.25,.25) {\footnotesize{L4}};
\node at (2.75,.25) {\footnotesize{L5}};
\node at (3.25,.25) {\footnotesize{L6}};
\node at (3.75,.25) {\footnotesize{L7}};


\node at (+0.25+4.75,+1.75) {\footnotesize{L0}};
\node at (+0.75+4.75,+1.75) {\footnotesize{H0}};
\node at (1.25+4.75,+1.75) {\footnotesize{L2}};
\node at (1.75+4.75,+1.75) {\footnotesize{H2}};
\node at (2.25+4.75,+1.75) {\footnotesize{L4}};
\node at (2.75+4.75,+1.75) {\footnotesize{H4}};
\node at (3.25+4.75,+1.75) {\footnotesize{L6}};
\node at (3.75+4.75,+1.75) {\footnotesize{H6}};

\node at (+0.25+4.75,+.25) {\footnotesize{L1}};
\node at (+0.75+4.75,.25) {\footnotesize{H1}};
\node at (1.25+4.75,.25) {\footnotesize{L3}};
\node at (1.75+4.75,.25) {\footnotesize{H3}};
\node at (2.25+4.75,.25) {\footnotesize{L5}};
\node at (2.75+4.75,.25) {\footnotesize{H5}};
\node at (3.25+4.75,.25) {\footnotesize{L7}};
\node at (3.75+4.75,.25) {\footnotesize{H7}};


\node at (+0.25+4.75+4.75,+1.75) {\footnotesize{L0}};
\node at (+0.75+4.75+4.75,+1.75) {\footnotesize{L1}};
\node at (1.25+4.75+4.75,+1.75) {\footnotesize{H0}};
\node at (1.75+4.75+4.75,+1.75) {\footnotesize{H1}};
\node at (2.25+4.75+4.75,+1.75) {\footnotesize{L2}};
\node at (2.75+4.75+4.75,+1.75) {\footnotesize{L3}};
\node at (3.25+4.75+4.75,+1.75) {\footnotesize{H2}};
\node at (3.75+4.75+4.75,+1.75) {\footnotesize{H3}};

\node at (+0.25+4.75+4.75,+.25) {\footnotesize{L4}};
\node at (+0.75+4.75+4.75,.25) {\footnotesize{L5}};
\node at (1.25+4.75+4.75,.25) {\footnotesize{H4}};
\node at (1.75+4.75+4.75,.25) {\footnotesize{H5}};
\node at (2.25+4.75+4.75,.25) {\footnotesize{L6}};
\node at (2.75+4.75+4.75,.25) {\footnotesize{L7}};
\node at (3.25+4.75+4.75,.25) {\footnotesize{H6}};
\node at (3.75+4.75+4.75,.25) {\footnotesize{H7}};

\node at (4.4,1) {\footnotesize{Trn}};
\node at (4.4+4.75,1) {\footnotesize{Zip}};

\end{tikzpicture}

\captionof{figure}{Transpose and zip instructions NEON}
\label{fig:shuffle2}
\end{figure}

The output from stage three is two vectors that are sorted in ascending order, so in one vector the two 64-bit subsets is reversed for the input of stage four, stage four is shown on Figure \ref{fig:stage4input}. These vectors are compared and produces a min and max vectors that are shuffled so that the right lanes are in the right place, by concatenating the low lanes of the min register with the low lanes of the max register and the same for the high lanes for the registers. The rest of stage four is exactly the same as stage three, the output from this is one eight lane vector that has the high values and one eight lane vector that has the low values, sorted in ascending order. The four lowest values are the zero padded values, so these are removed.

\begin{figure}[h!]
\centering
\begin{tikzpicture}

\filldraw[fill=YellowGreen] (-1,.75) rectangle (3,1.25);

\draw[step=0.5cm,color=gray] (-1,.75) rectangle (-.5,1.25);
\draw[step=0.5cm,color=gray] (-.5,.75) rectangle (0,1.25);
\draw[step=0.5cm,color=gray] (0,.75) rectangle (.5,1.25);
\draw[step=0.5cm,color=gray] (.5,.75) rectangle (1,1.25);

\draw[step=0.5cm,color=gray] (1,.75) rectangle (1.5,1.25);
\draw[step=0.5cm,color=gray] (1.5,.75) rectangle (2,1.25);
\draw[step=0.5cm,color=gray] (2,.75) rectangle (2.5,1.25);
\draw[step=0.5cm,color=gray] (2.5,.75) rectangle (3,1.25);
\node at (-0.75,+1) {\footnotesize{i0}};
\node at (-0.25,+1) {\footnotesize{i1}};
\node at (+0.25,+1) {\footnotesize{i2}};
\node at (+0.75,+1) {\footnotesize{i3}};
\node at (1.25,+1) {\footnotesize{i4}};
\node at (1.75,+1) {\footnotesize{i5}};
\node at (2.25,+1) {\footnotesize{i6}};
\node at (2.75,+1) {\footnotesize{i7}};

\filldraw[fill=ProcessBlue] (-1,.0) rectangle (3,.5);

\draw[step=0.5cm,color=gray] (-1,0) rectangle (-.5,.5);
\draw[step=0.5cm,color=gray] (-.5,.0) rectangle (0,.5);
\draw[step=0.5cm,color=gray] (0,.0) rectangle (.5,.5);
\draw[step=0.5cm,color=gray] (.5,.0) rectangle (1,.5);

\draw[step=0.5cm,color=gray] (1,0) rectangle (1.5,.5);
\draw[step=0.5cm,color=gray] (1.5,.0) rectangle (2,.5);
\draw[step=0.5cm,color=gray] (2,.0) rectangle (2.5,.5);
\draw[step=0.5cm,color=gray] (2.5,.0) rectangle (3,.5);
\node at (-0.75,+0.25) {\footnotesize{j0}};
\node at (-0.25,+0.25) {\footnotesize{j1}};
\node at (+0.25,+0.25) {\footnotesize{j2}};
\node at (+0.75,+0.25) {\footnotesize{j3}};
\node at (1.25,+0.25) {\footnotesize{j4}};
\node at (1.75,+0.25) {\footnotesize{j5}};
\node at (2.25,+0.25) {\footnotesize{j6}};
\node at (2.75,+0.25) {\footnotesize{j6}};


\draw[step=0.5cm,color=gray] (-1,-.75) rectangle (-.5,-.25);
\draw[step=0.5cm,color=gray] (-.5,-.75) rectangle (0,-.25);
\draw[step=0.5cm,color=gray] (0,-.75) rectangle (.5,-.25);
\draw[step=0.5cm,color=gray] (.5,-.75) rectangle (1,-.25);

\draw[step=0.5cm,color=gray] (1,-.75) rectangle (1.5,-.25);
\draw[step=0.5cm,color=gray] (1.5,-.75) rectangle (2,-.25);
\draw[step=0.5cm,color=gray] (2,-.75) rectangle (2.5,-.25);
\draw[step=0.5cm,color=gray] (2.5,-.75) rectangle (3,-.25);
\node at (-0.75,-.5) {\footnotesize{H0}};
\node at (-0.25,-.5) {\footnotesize{H1}};
\node at (+0.25,-.5) {\footnotesize{H2}};
\node at (+0.75,-.5) {\footnotesize{H3}};
\node at (1.25,-.5) {\footnotesize{H4}};
\node at (1.75,-.5) {\footnotesize{H5}};
\node at (2.25,-.5) {\footnotesize{H6}};
\node at (2.75,-.5) {\footnotesize{H7}};


\draw[step=0.5cm,color=gray] (-1,-1.5) rectangle (-.5,-1);
\draw[step=0.5cm,color=gray] (-.5,-1.5) rectangle (0,-1);
\draw[step=0.5cm,color=gray] (0,-1.5) rectangle (.5,-1);
\draw[step=0.5cm,color=gray] (.5,-1.5) rectangle (1,-1);

\draw[step=0.5cm,color=gray] (1,-1.5) rectangle (1.5,-1);
\draw[step=0.5cm,color=gray] (1.5,-1.5) rectangle (2,-1);
\draw[step=0.5cm,color=gray] (2,-1.5) rectangle (2.5,-1);
\draw[step=0.5cm,color=gray] (2.5,-1.5) rectangle (3,-1);
\node at (-0.75,-1.25) {\footnotesize{L0}};
\node at (-0.25,-1.25) {\footnotesize{L1}};
\node at (+0.25,-1.25) {\footnotesize{L2}};
\node at (+0.75,-1.25) {\footnotesize{L3}};
\node at (1.25,-1.25) {\footnotesize{L4}};
\node at (1.75,-1.25) {\footnotesize{L5}};
\node at (2.25,-1.25) {\footnotesize{L6}};
\node at (2.75,-1.25) {\footnotesize{L7}};


\node at (3.25,.6) {$<$};
\node at (3.25,-.2) {$=$};

\node at (-1.3,1) {V0};
\node at (-1.3,.25) {V1};
\node at (-1.5,-.5) {High};
\node at (-1.5,-1.25) {Low};

%%Shuffle
\draw[->] (.75,-1.75) .. controls (.75,-2.2) and (1.25,-1.8) .. (1.25,-2.25);
\draw[->] (1.25,-1.75) .. controls (1.25,-2.2) and (.75,-1.8).. (.75,-2.25);

%% Result bottpm  left

\draw[step=0.5cm,color=gray] (-1,-3) rectangle (-.5,-2.5);
\draw[step=0.5cm,color=gray] (-.5,-3) rectangle (0,-2.5);
\draw[step=0.5cm,color=gray] (0,-3) rectangle (.5,-2.5);
\draw[step=0.5cm,color=gray] (.5,-3) rectangle (1,-2.5);

\draw[step=0.5cm,color=gray] (1,-3) rectangle (1.5,-2.5);
\draw[step=0.5cm,color=gray] (1.5,-3) rectangle (2,-2.5);
\draw[step=0.5cm,color=gray] (2,-3) rectangle (2.5,-2.5);
\draw[step=0.5cm,color=gray] (2.5,-3) rectangle (3,-2.5);
\node at (-0.75,-2.75) {\footnotesize{L0}};
\node at (-0.25,-2.75) {\footnotesize{L1}};
\node at (+0.25,-2.75) {\footnotesize{l2}};
\node at (+0.75,-2.75) {\footnotesize{l3}};
\node at (1.25,-2.75) {\footnotesize{H0}};
\node at (1.75,-2.75) {\footnotesize{H1}};
\node at (2.25,-2.75) {\footnotesize{H2}};
\node at (2.75,-2.75) {\footnotesize{H3}};

\draw[step=0.5cm,color=gray] (-1,-3.75) rectangle (-.5,-3.25);
\draw[step=0.5cm,color=gray] (-.5,-3.75) rectangle (0,-3.25);
\draw[step=0.5cm,color=gray] (0,-3.75) rectangle (.5,-3.25);
\draw[step=0.5cm,color=gray] (.5,-3.75) rectangle (1,-3.25);

\draw[step=0.5cm,color=gray] (1,-3.75) rectangle (1.5,-3.25);
\draw[step=0.5cm,color=gray] (1.5,-3.75) rectangle (2,-3.25);
\draw[step=0.5cm,color=gray] (2,-3.75) rectangle (2.5,-3.25);
\draw[step=0.5cm,color=gray] (2.5,-3.75) rectangle (3,-3.25);
\node at (-0.75,-3.5) {\footnotesize{L4}};
\node at (-0.25,-3.5) {\footnotesize{L5}};
\node at (+0.25,-3.5) {\footnotesize{L6}};
\node at (+0.75,-3.5) {\footnotesize{L7}};
\node at (1.25,-3.5) {\footnotesize{H4}};
\node at (1.75,-3.5) {\footnotesize{H5}};
\node at (2.25,-3.5) {\footnotesize{H6}};
\node at (2.75,-3.5) {\footnotesize{H7}};


\node at (4.5,-3.1) {Input stage 3};

\node at (-1.3,-2.75) {V0};
\node at (-1.3,-3.75) {V1};


\end{tikzpicture}

\captionof{figure}{Stage 4 merging network}
\label{fig:stage4input}
\end{figure}
\FloatBarrier
Listing \ref{lst:binningmedian} shows the code for how capturing a image and binning is done. This was concluded to be the fastest way to do it in the specialization project. In the \textit{getMedianValue} function 12 sample values get read into their vectors, sorted and written back to memory. The middle value, six, from the sorted values is the resulting median value. Each row from the sensor is independent from each other so two threads can work on different rows. The time it takes to execute line 4 to 13 is whats measured here, this is the time it takes to binn one image. The average time for this is 27 ms and 55 ms with only one thread, when the maximum framerate is set for the resolutions presented in table \ref{tab:fpsresults2}.



\begin{lstlisting}[language=C++, caption={Median Binning}, label={lst:binningmedian}]
for(int imageNumber=0; imageNumber<nSingleFrames; imageNumber++){
    is_WaitForNextImage(hCam, 1000, &(rawImageP), &imageSequenceID);

    #pragma omp parallel for num_threads(2)
    for(int row=0; row<sensorRows; row++){
        int rowOffset = row*sensorColumns;
        int binnedIdxOffset = row*nBandsBinned;

        int binOffset = 0;
        for(int binnIterator=0; binnIterator<nFullBinnsPerRow; binnIterator++){
            binnedImages[imageNumber][binnedIdxOffset+binnIterator = getMedianValue((unsigned char*)(&rawImageP), rowOffset+binOffset, rowOffset+binOffset+12, 6);
        }
    }
    is_UnlockSeqBuf (hCam, 1, rawImageP);
}
\end{lstlisting}
\FloatBarrier
\section{FPS and resolution}
\label{chap:TestingFPS}
Testing to find the maximum framerates and resolutions supported by the GigE camera is performed. The binning stage in Figure \ref{fig::design2} is not included, so that the only work the CPU is doing is using the driver to write images to memory and fetching a pointer to that memory in the program and freeing the memory again. To measure the framerate, the time it takes from the moment \textit{is\_WaitForNextImage} function have returned a pointer until the next time the function returns a pointer is recorded. The \textit{is\_WaitForNextImage} function does not return unless a frame is ready, it has timed out or an error has occured. Table \ref{tab:fpsresults} shows the framerate achieved for different resolutions at different bit-depths.

\begin{table}[h!]
\caption{Maximum framerate results}
\label{tab:fpsresults}

\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\rowcolor{Lightgray}\textsf{FPS}   & \begin{tabular}[c]{@{}l@{}}\textsf{1936x}\\ \textsf{1216}\end{tabular} &
         \begin{tabular}[c]{@{}l@{}}\textsf{1920x}\\ \textsf{1080}\end{tabular} &
         \begin{tabular}[c]{@{}l@{}}\textsf{1600x}\\ \textsf{1200}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textsf{1280x}\\ \textsf{1024}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textsf{1024x}\\ \textsf{1024}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textsf{1280x}\\ \textsf{720}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textsf{1024x}\\ \textsf{768}\end{tabular} \\ \hline
16-bit GigE &  6 & 9 & 9 & 16 & 25 & 28 & 32 \\ \hline
8-bit GigE & 20  & 25 & 27& 32 & 45 & 47 & X \\ \hline
8-bit USB &  14 & 13 & 12& 26 & 34 &  47& X \\ \hline

\end{tabular}
\end{table}


Both USB and GigE cameras were tested. For USB, the driver constrained the framerate of the camera itself, when setting the framerate, it's only possible to set a maximum value for the framerate, setting anything above that limit results in the camera using the maximum value. For GigE there is no such constraints, it is possible to set any framerate, but when the framerate is set too high, the \textit{is\_WaitForNextImage} function returns a empty pointer and the driver prints that an error occurred.

The two last results for 8-bit GigE in Table \ref{tab:fpsresults} of 47 FPS is due to the fact that the maximum framerate that the camera can deliver is 47 FPS. The CPU load averages around 94\% for the first 16-bit test, that is, resolution 1936x1216. The CPU load decreases to 60-70\% and 50\% for 1920x1080 and 1600x1200 respectively, then for all other resolutions in 16-bit and 8-bit, the CPU load is around 40\%.

%Subsampling the pixels in horizontal direction on 1936x1216 resolution has also been tested. The FPS for subsampling 2x gives a FPS of 23 and with 4x gives 32 FPS. When assuming that the 12-bit samples is transferred in 16-bit words, the scatter plot on figure \ref{fig:scatterfps} shows the bandwidth for the whole system, from capturing frames to the frame being written to memory. Excluding the three values where the CPU is working harder, the average bandwidth is 48.4 MBps.

%Only sampling in the spectral direction has been done because it was not possible to do it in spatial direction, said sivert.
%Why these exact binning facors? because sivert wanted it.

Testing the same resolutions with the binning stage included gives the framerates shown in Table \ref{tab:fpsresults2}. A * in the result means that the driver constrains the framerate. Some of the tests are run with subsampling, where subsampling is done in only the spectral direction, not the spatial deirection of the sensor. The reason for this is because the HYPSO mission judged it unacceptable to reduce the resolution in this dimension.

% Subsampled values with mean binning in the regular CPU core(not SIMD) are on entry 4 and 5, and subsampling with the binning stage cut out are on entry 6 and 7 in the table. It is desirable to have the same number of bins when the subsampling is used too, therefore the number of samples in each bin is reduced to 6 when subsampling 2x is used and 3 when subsampling 4x is used, this proved to be so small bins that it wasnt any point in using SIMD for this.

It was also requested from the HYPSO team that mean binning not in SIMD(regular CPU, or SISD(Single Instruction Single Data)) with and without subsampling, theese results are in the three first entries of the table. A test with mean binning in SIMD is also shown for comparison. How the mean binning is carried out can be read about in the specialization project\cite{specializationproject}. Three tests for median binning in SIMD, with different subsamlpings and binning factors are tested and two tests with subsampling where no work on the CPU is done have been tested.



\begin{table}[h!]
\centering
\caption{FPS: 16-bit GigE, binning}
\label{tab:fpsresults2}

\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline


        \rowcolor{Lightgray}\textsf{Method}   & \begin{tabular}[c]{@{}l@{}}\textsf{1936x}\\ \textsf{1216}\end{tabular} &
         \begin{tabular}[c]{@{}l@{}}\textsf{1920x}\\ \textsf{1080}\end{tabular} &
         \begin{tabular}[c]{@{}l@{}}\textsf{1600x}\\ \textsf{1200}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textsf{1280x}\\ \textsf{1024}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textsf{1024x}\\ \textsf{1024}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textsf{1280x}\\ \textsf{720}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textsf{1024x}\\ \textsf{768}\end{tabular} &
          \begin{tabular}[c]{@{}l@{}}\textsf{800x}\\ \textsf{600}\end{tabular}\\ \hline

Mean SISD bin12  &  1 & 1 & 3 & 9 & 12 & 15 & 22 & 32*\\ \hline
Mean SISD bin6 sub2x &  15 & 18 & 20 & 30 & 37 & 42 & 47 & X\\ \hline
Mean SISD bin3 sub4x &  32* & 36* & 33* & 37* & 37* & 47 & X & X\\ \hline
Mean SIMD bin12 &  3 & 6 & 12 & 16 & 19 & 21 & 27 & 32*\\ \hline
Median SIMD bin12 &  5 & 5 & 9 & 11 & 17 & 19 & 23 & 32*\\ \hline
Median SIMD bin8 sub2x &  16 & 18 & 21 & 30 & 36 & 44 & 47 & X\\ \hline
Median SIMD bin4 sub4x &  32* & 35* & 32* & 37* & 38* & 47* & X & X\\ \hline
Do nothing sub2x &  23 & 26 & 30 & 37* & 37* & 47 & X & X\\ \hline
Do nothing sub4x &  32* & 36* & 33* & 37* & 37* & 47 & X & X\\ \hline
\end{tabular}
\end{table}

These exact subsambling and binning factors have been tested because this is what the HYPSO mission wanted. The median binning is done in the same way as described in chapter \ref{chap::binningtest}. For binning factor 6, the last stage of figure \ref{fig::bitonicmergenetwork} is removed, it then becomes a 8 input bitonic merging network, where zero padding is added to the inputs. For binning factor 4, stage 3 and 4 of the network is removed. For binning factor 3, in addition to removal of the stages 3 and 4, zero padding is required. The CPU load under testing of the framerates has been very fluctuating for both mean and median, it fluctuated between 60\%-80\% for all resolutions, except for the two highest resolutions, where the CPU load stays above 90\%.
%CPU fluctuates between 60-80% when binning with SIMD for both median and mean
%AVG binning time, subsampling 2x = 14ms
%AVG binning time, subsampling 4x = 18ms
It is possible to set the resolution up in two ways with the driver, as described in chapter \ref{chap::ueye} one way is with the \textit{is\_AOI} function and one is with the \textit{is\_ImageFormat} function. The values in table \ref{tab:fpsresults} and \ref{tab:fpsresults2} are obtained with the {is\_ImageFormat} method. Testing have also been done to see if the \textit{is\_AOI} method or the \textit{is\_ImageFormat} method is faster when capturing frames, this is not the case, both methods yields the same result.


Figure \ref{fig::bps} shows the throughput in MB for the framerates achieved in table \ref{tab:fpsresults} for the 8- and 16-bit GigE interface. The values are calculated by multiplying number of pixels per frame by frames per second by number of bytes per pixels. The throughput is relatively stable for 8-bit and for 16-bit the throughput increases as the resolution goes down for the four first values.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/bps.png}
	\caption{Thougput maximum framerate }
	\label{fig::bps}
\end{figure}
\newpage
\section{Trigger mode}

To see if there is any difference in stability and CPU load between using HW-Trigger and freerun mode, a signal generator is connected to pin four and seven of the camera. Testing is performed by recording the time between each frame is captured, the same way as in chapter \ref{chap:TestingFPS}. When using the same resolutions and FPS as in table \ref{tab:fpsresults} for 16-bit images, there is no noticeable difference in CPU utilization or imagecapture stability between HW-Trigger and freerun mode. Figure \ref{fig:triggerresults} shows the timing results for resolutions 1280x720 and 1936x1216. There is a difference between maximum resolution with maximum stable framerate and lower resolutions. As shown in figure \ref{subfig-2:19361216}, the time between frames becomes much more unstable, compared to figure \ref{subfig-1:1280720}. The raw data for the result can be viewed online at this thesis github\cite{gitthesis}.

\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{\textwidth}
	  \includegraphics[width=\textwidth]{images/Triggertest1.PNG}
       \caption{1280x720}
       \label{subfig-1:1280720}
     \end{subfigure}

   \begin{subfigure}[b]{\textwidth}

	  \includegraphics[width=\textwidth]{images/Triggertest2.PNG}
       \caption{1936x1216}
       \label{subfig-2:19361216}
     \end{subfigure}
     \caption{Trigger test results}
     \label{fig:triggerresults}
\end{figure}



\FloatBarrier
\chapter{Processing pipeline}
\label{chap::pipeline}
A design for how to capture and compress images have been made, here is a short explanation on how the process of getting a image pipeline up and running was, and a explanation of how the design looks like and how the different modules in the design works. A more detailed tutorial for how to get the pipeline up and running is included as an appendix.

One of the first steps of the process of getting the pipeline to work was to first get the cubeDMA implementation running with no OS and to get to know the design, then build it with Linux and get it to run the same way there. Because Linux is a virtual memory system, it swaps pages in and out of memory. Therefore, a driver that handles DMA memory had to be made for cubeDMA to work with Linux. Afterwards the implementation of CCSDS-123 was inserted into the pipeline and verified. Then SW-modules for image capturing and CubeDMA communication were made so the complete hyperspectral image pipeline could be tested to work together.

\section{Implementing CubeDMA}
To implement CubeDMA for the Zynq system, Vivados block design feature is used. A CubeDMA block can be added by dragging and dropping from RTL design, but this can turn out to be a mistake. In Vivado version 2018.3, when this is done, the generic parameters in the CubeDMA design are set to their default values, regardless of specification in the block design. This might not be an issue in other versions of Vivado. For this not to happen, the RTL design must be exported as an IP from Vivado and then imported to the block design. Once this is done, the parameters for axis width, component width and number of components can be set. To use BSQ order transfer, tinymover must also be enabled.




For testing purposes, a FIFO is used, this has to be the same width as CubeDMA and be connected between \textit{s\_axis\_s2mm} channel and \textit{m\_axis\_mm2s}. The Zynq processing system 7 core must be added, this core can manage the peripheral I/O connections. It is important that Ethernet, USB, SD and UART are connected to MIO pins, for the PS to function. MIO pins are the pins which connects the peripheral devices to the I/O mux on figure \ref{fig::soc}. Figure \ref{fig::blockdesign} shows the block design of the system

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/blockdesign.png}
	\caption{Block design Vivado}
	\label{fig::blockdesign}
\end{figure}

\section{Building Linux}
The hardware design made in Vivado must be exported and added in petalinux tools, the design will then be uploaded to the FPGA when the system boots.

A device tree is a data structure that holds information about the hardware the system is runing on, this is read by the kernel when booting. Included in this tree is the size of RAM, it is not possible to change RAM size or anything other in the device tree with Petalinux. However it is possible to add nodes into the device tree. One node type that can be added is a reserved memory node, this node reserves a range of memory from a starting address, this range will not be mapped to the kernels virtual address space, user virtual address space or be used in any way by the kernel. After this is added the boot images can be built and the system can be booted.

\section{Kernel module}
\label{chap::kernelmodule}

The driver written is essentially two instances (or two devices) of one driver, when it is first initialized it creates two different kernel modules, one for sending data and one for receiving data. The memory reserved from being used by the kernel can be mapped to user space with the \textit{mmap} function by the kernel, how this is done is shown in listing \ref{lst:usmap}.

\begin{lstlisting}[language=C++, caption={User mapping}, label={lst:usmap}]
static int dev_mmap(struct file *file_p, struct vm_area_struct *vma){
	struct inode *inode = (struct inode *)file_p->private_data;

	if(imajor(inode)==dma_channel[0].major_number){//send
		remap_pfn_range(vma, vma->vm_start, SEND_PHYS_ADDR>>PAGE_SHIFT, vma->vm_end-vma->vm_start, vma->vm_page_prot);
	}
	else{//reieve
		remap_pfn_range(vma, vma->vm_start, RECIEVE_PHYS_ADDR>>PAGE_SHIFT, vma->vm_end-vma->vm_start, vma->vm_page_prot);
	}
	return 0;
}
\end{lstlisting}

The first task the mmap function performs is to copy the content from \textit{private\_data} in the file structure to a inode structure. The \textit{private\_data} field of this structure is initially a NULL pointer. This have been assigned to point to the inode structure of the driver, when it was first opened. Listing \ref{lst:opendriv} shows how the the drivers open function looks like.

Then in line 4 of listing \ref{lst:usmap} the inode is used to find the major number of this driver instance to decide if it should map the receiver address or sender address to memory. The function that does the actual mapping of memory, is the function in line 5 or 8 based on if it is the receiver or sender. The function builds a page table of user space virtual addresses, for a range of physical addresses and passes it to user space. Many of the arguments for this function is provided from the kernel, when mmap is called, the arguments passed to \textit{remap\_pfn\_range} are:

\begin{tcolorbox}[coltitle=black, colframe=gray!50, title={int \textsf{remap\_pfn\_range}(struct vm\_area\_struct* \textbf{vma}, unsigned \textbf{long virt\_addr}, unsigned long \textbf{pfn}, unsigned long \textbf{size}, pgprot\_t \textbf{prot})}]

\textbf{vma} - The virtual memory area the pages are being mapped.

\textbf{virt\_addr} - Starting address of the virtual address range

\textbf{pfn} - The frame number of the first frame in memory that the virtual address range is being mapped to. This is the physical address right shifted by how many bits that are in the page index. Since pages and frames are equal length.

\textbf{size} - Size of the memory being mapped.

\textbf{prot} - The protection level of the new virtual memory.

\end{tcolorbox}






\begin{lstlisting}[language=C++, caption={Open driver}, label={lst:opendriv}]
static int dev_open(struct inode *inodep, struct file *filep){
	filep->private_data = inodep;
	return 0;
}
\end{lstlisting}

Because the data is cached, there has to be a way to make sure that the correct data is in RAM when DMA starts to transfer and that the right data is read when reading the data in the processing system after DMA has transferred back to RAM. To make sure of this the caches can be flushed and cleaned. In ARM termonology, flushing cache means making the data that is in the caches invalid,    the next time the CPU accesses this data, it has to read it from RAM, not cache. Cleaning a cache means to make data in cache dirty, the data in cache is then written to memory. Before the data can be flushed and cleaned, the adresses has to be available to the kernel. This is done with the function in listing \ref{lst:kernrem}, this function maps physical adddresses to kernel virtual addresses.

\begin{lstlisting}[language=C++, caption={Kernel remapping}, label={lst:kernrem}, numbers=none]
memremap(SEND_PHYS_ADDR, sizeof(struct dma_data), MEMREMAP_WB, numbers=none);
\end{lstlisting}

Listing \ref{lst:flush} Shows how data gets flushed and cleaned. The function is a ioctl function, which is a system call that can be used with char drivers to do driver specific things, and in this driver it cleans and flushes cache. The \textit{cmd} argument decides if a clean of the send and receive addresses or flush flush of the receive addresses should be done. The function in line 3 cleans and flushes the L1 cache and the function on line 4 cleans L2 cache and the function on line 7 flushes L2 cache. The order of when theses functions are called depending on what direction the data has to flow.

\begin{lstlisting}[language=C++, caption={Flush and invalidate}, label={lst:flush}]
static long dev_ioctl(struct file* file, unsigned int cmd, unsigned long arg){
	if(cmd==0){//Clean send cache
		__cpuc_flush_dcache_area(dma_channel[0].p_dma_data, sizeof(struct dma_data));
		outer_clean_range(SEND_PHYS_ADDR, 0x19000000);
	}
	else{//Invalidate recieve
		outer_inv_range(RECIEVE_PHYS_ADDR, 19000000);
		__cpuc_flush_dcache_area(dma_channel[1].p_dma_data, sizeof(struct dma_data));
	}
	return 0;
}
\end{lstlisting}

\section{User space}
\label{chap::userspace}
From user space the driver can be opened the same way as a file, shown on line 1 and 2 in listing \ref{lst:usrspc}. Since the driver is initialized with two instances, both has to be opened. The mmap functions on line 4 and 6 gets a pointer to the virtual memory mapped from the driver. These pointers can now be used to read and write to the addresses reserved from the kernel.


\begin{lstlisting}[language=C++, caption={Map memory to user space}, label={lst:usrspc}]
int fd_send = open("/dev/cubedmasend", O_RDWR);
int fd_recieve = open("/dev/cubedmareceive", O_RDWR);

send_channel = (struct dma_proxy_channel_interface *)mmap(NULL, sizeof(struct dma_proxy_channel_interface), PROT_READ | PROT_WRITE, MAP_SHARED, fd_send, 0);

recieve_channel = (struct dma_proxy_channel_interface *)mmap(NULL, sizeof(struct dma_proxy_channel_interface), PROT_READ | PROT_WRITE, MAP_SHARED, fd_recieve, 0);
\end{lstlisting}

The memory mapped cubeDMA registers can be found in the mem file, opened and mapped to user space memory, as shown in listing \ref{lst:devusr}. This maps one page from base address of the device and casts it as a 32-bit pointer, this is 32-bit because the device registers are 32-bit. The base address for the device can be found in the address field of the device in vivado, this is usually 0x43c00000 or higher.

\begin{lstlisting}[language=C++, caption={Map device to user space}, label={lst:devusr}]
fd = open("/dev/mem", O_RDWR|O_SYNC)
deviceMem = static_cast<uint32_t*>(mmap(0, getpagesize(), PROT_READ|PROT_WRITE, MAP_SHARED, fd, CUBEDMA_BASE))

\end{lstlisting}

\section{Implementing CCSDS-123}

To implement CCSDS-123, the FIFO in figure \ref{fig::blockdesign} is switched out with the CCSDS123 module. The module can either be added by importing the source files or having it made into an IP and imported to the project the same way as cubeDMA is. The samples from the camera use unsigned values, so the CCSDS-123 must also use this. The on the fly option must be un-checked when setting up, if on the fly processing is enabled and the cubeDMA is delivering data slower or faster than the CCSDS-123 module reads it, the result will become corrupted. Number of pipelines, bit-depth and bus width must match with axis width, number of components and component width for both MM2S and S2MM on the cubeDMA block. The rest of the parameters is up to the user. Figure \ref{fig::utilization} shows how much of the FPGA's resources is utilized when the design is implemented on the Zedboard.

\begin{figure}[!htb]
     \centering

       \begin{subfigure}[b]{\linewidth}
	    %\rule{\linewidth}{\linewidth}
	    \includegraphics[width=\textwidth]{images/utilizationall.png}
         \caption{Utilization FPGA design}
         \label{fig::utilizationall}
       \end{subfigure}\\[\baselineskip]
       \begin{subfigure}[b]{\linewidth}
	    %\rule{\linewidth}{\linewidth}
	    \includegraphics[width=\textwidth]{images/utilizationallgraph.png}
         \caption{Utilization graph}
         \label{fig::utilizationallgraph}
       \end{subfigure}
     \caption{Utilization FPGA}
     \label{fig::utilization}
   \end{figure}

The endianess of the input data has to be little endian. To test that everything compresses and transfers correctly, the raw data is also stored on the board, then it is compressed by a software compressor called Emporda. The Emporda software is a java implementation of the CCSDS-123 algorithm, it is open source, easy to use and has manual on how to use it \cite{emporda}. Since this is a lossless compression algorithm the Emporda result should be the same as the FPGA result, these two results is then compared. It should be noted that the user defined parameters must be the same for the FPGA implementation and Emporda for the results to be the same.

\section{System design}
\label{chap::systemdesign}
Figure \ref{fig::systemov} shows the systems modules and how they fit together. In the processing system the HSICamera module uses the camera driver to capture frames, make a cube and store it to memory. The DMA memory manager handles DMA memory, like it is described in chapter \ref{chap::kernelmodule} and the CDMA driver handles interfacing with CubeDMA. In programmable logic, CubeDMA fetches a cube from memory, sends it to CCSDS-123 module which compresses the image sends it back to CubeDMA which in turn sends it back to memory again.

\begin{figure}
\begin{tikzpicture}

\node [rectangle, fill = white, draw=black, minimum height = 6cm, text width = 7cm, rounded corners] (sw) at (6,-1.5) {};
\node[] at (6,1.2) {PS};

\node [rectangle, fill = white, draw=black, minimum height = 3cm, text width = 7cm, rounded corners] (hw) at (6,-6) {};
\node[] at (6,-4.8) {PL};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (camera) at (0,0) {Camera};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (driver) at (4,0) {Driver};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (hsi) at (8,0) {HSICamera};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (cdmadriver) at (4,-3) {CDMA Driver};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (dmamem) at (8,-3) {DMA Mem Manager};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (ccsds) at (4,-6) {CCSDS123};

\node [rectangle, fill = hue, draw=black, minimum height = 2cm, text width = 2cm, rounded corners, text centered] (cubdma) at (8,-6) {CubeDMA};


\node [rectangle, fill = hue, draw=black, minimum height = 10cm, text width = 2cm, text centered] (mem) at (12,-3) {Memory};


\path [draw, ->, thick, -latex] (driver) -- (camera);
\path [draw, ->, thick, -latex] (camera) -- (driver);
\path [draw, ->, thick, -latex] (driver) -- (hsi);
\path [draw, ->, thick, -latex] (hsi) -- (driver);
\path [draw, ->, thick, -latex] (hsi) -- (10.8, 0);
\path [draw, ->, thick, -latex] (hsi) to [out=-120,in=20] (cdmadriver);
\path [draw, ->, thick, -latex] (hsi) -- (dmamem);
\path [draw, ->, thick, -latex] (dmamem) -- (mem);
\path [draw, ->, thick, -latex] (cdmadriver) to [out=-10,in=120] (cubdma);
\path [draw, ->, thick, -latex] (ccsds) -- (cubdma);
\path [draw, ->, thick, -latex] (cubdma) -- (ccsds);
\path [draw, ->, thick, -latex] (10.8, -6) -- (cubdma);
\path [draw, ->, thick, -latex] (cubdma) -- (10.8, -6);

% \path [draw, ->, thick, -latex] (program) -- (mem);
% \path [draw, ->, thick, -latex] (mem) -- (program);
% \path [draw, ->, thick, -latex] (program) to[out=270,in=180] (storage);
% \path [draw, ->, thick, -latex] (mem) -- (fpga);
%\path [draw, ->, -latex, dashed] (tekst) -- (6.4, -1.1);
\end{tikzpicture}
\caption{Block design processing pipeline}
\label{fig::systemov}
\end{figure}


The HSICamera and CDMADriver module are C++ classes and DMAMemManager is a kernel module, a UML of these modules are shown on figure \ref{fig::uml1}, all the code written can be found on github \cite{gitthesis}. The HSICamera class instantiates the CubeDMADriver class and opens the DMAMemoryManager driver. The \textit{SharedMemParams} is a header file that contains data that is shared between HSICamera and DMAMemoryManager, this is the physical address of where DMA is recieveing and sending, and a structure for holding the DMA data.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\textwidth]{images/uml.png}
	\caption{UML processing pipeline}
	\label{fig::uml1}
\end{figure}
\FloatBarrier
\subsection{CubeDMADriver}

The driver for cube DMA transfer has a init fuction that has to be run before it can be used for transfer, what it is doing is it opens the device memory file in Linux and maps the device address as a 32-bit pointer to the driver, like described in chapter \ref{chap::userspace}. It then initializes the registers with a initialization struct that is passed to the function. The initialization struct contains the fields shown in table \ref{tab::mm2s} and \ref{tab::S2MM}. It has functions for starting a transfer and polling the device registers to check if transfer is finished for the receive and send channels of CubeDMA or check if a interrupt has occurred.

\subsection{HSICamera}

The AOI method of setting the resolution is used, this is because it gives the freedom to use any resolution, it takes an width and height, then sets the AOI to be in the middle of the sensor.

The position and size of the AOI is specified in a struct that is passed to the \textit{is\_AOI} function, the x and y position in this struct sets the x and y position of the upper left corner of the AOI on the sensor. The x and y position changes based on the size of the AOI, this is calculated with equation \ref{eq:aoi}, figure \ref{fig::aoisensor} shows the parameters used in the equation.

\begin{equation}
\label{eq:aoi}
X_{offset} = \frac{Sensor width-Width}{2}
Y_{offset} = \frac{Sensor height-Height}{2}
\end{equation}

\begin{figure}
    \centering
    \begin{tikzpicture}
    \draw[step=0.5cm,color=gray] (-1, -1) grid (5, 5);
    \draw[very thick, step=0.5cm,color=black] (-1, -1) rectangle (5, 5);
    \draw[thick, step=0.5cm,color=black] (1, 1) rectangle (3, 3);

    \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=4pt]
    (-1, 5) -- (1, 5) node [black,midway,xshift=-0cm, yshift=15pt]
    {\footnotesize $X_{offset}$};

    \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt]
    (-1, 3) -- (-1, 5) node [black,midway,xshift=-0.6cm,rotate=60]
    {\footnotesize $Y_{offset}$};

    \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=4pt]
    (1, 3) -- (3, 3) node [black,midway,xshift=-0cm, yshift=15pt]
    {\footnotesize Width};

    \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt]
    (1, 1) -- (1, 3) node [black,midway,xshift=-0.6cm,rotate=60]
    {\footnotesize Height};

    \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-4pt]
    (5, -1) -- (-1, -1) node [black, below, midway,xshift=-0cm, yshift=-5pt]
    {\footnotesize Sensor width};

    \draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=4pt},yshift=0pt]
    (5, -1) -- (5 , 5) node [black,midway,xshift=1.8cm] {\footnotesize
    Sensor height};

    \node at (2,2) {AOI};

    \end{tikzpicture}
    \caption{AOI sensor}
    \label{fig::aoisensor}
\end{figure}
\FloatBarrier
This struct also sets how the image is stored in memory, it can have absolute memory position on the X axis or Y axis. The memory allocated for the image can be bigger than the actual size of the AOI, the absolute memory position tells the driver how the image is stored within this memory. Figure \ref{fig::nonabs} shows how this looks like with no absolute memory position and \ref{fig::abs} shows how this looks like with absolute memory position on both axis.
\FloatBarrier
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{images/nonabsolutemem.png}
	\caption{Non absolute memory position\cite{ueyemanual}}
	\label{fig::nonabs}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{images/absolutemem.png}
	\caption{Absolute memory position\cite{ueyemanual}}
	\label{fig::abs}
\end{figure}
\FloatBarrier
In this design absolute memory position on both axes is used, with a memory that is 1936x1216 big, these offsets is accounted for when reading the image. The reason 1936x1216 is used, is because this is the sensor size and this gives the possibility to use all of the sensor as a AOI.

To capture a cube with the design two functions is used \textit{initialize} and \textit{run\_cube\_capture}. The \textit{initilize} function takes five arguments:

\begin{itemize}
    \item \textbf{Exposure.} The exposure time of the camera sensor in milli seconds.
    \item \textbf{Rows.} Number of rows in each frame.
    \item \textbf{Columns.} Number of columns in each frame.
    \item \textbf{Frames.} Number of frames in the cube.
    \item \textbf{FPS.} Framerate of the camera
    \item \textbf{Cube format.} Cube format is a enumerator used to specify what cube format to store the raw cube in, or to store it at all. Listing \ref{lst::cubeform} shows this enumerator, None mean to not store the cube.
    \begin{lstlisting}[language=C++, caption={Cube formats}, label={lst::cubeform}, numbers=none]
        enum cubeFormat { Bil, Bip, Bsq, Raw, None };
    \end{lstlisting}

\end{itemize}
The \textit{initialize} function initilizes the camera with the exposure time, FPS and sets the number of columns and rows for the frame in the AOI. The pixel clock, display mode and color mode is also set, but these are fixed parameters.

Variables for binning is calculated, variables such as; How many binns there are for each row of the sensor. How many samples there are for the last bin in each row, since this varies for different image sizes.

Memory allocation for camera frames is done and a image memory buffer of size ten is made, ten is used because this has been sufficient enough during development. The DMAMemoryManager is used to allocate memory for the cube and the device memory is mapped into user space as described in a previous chapters.

When the \textit{run\_cube\_capture} function is run, the program captures the frames, builds a cube and sends it to the FPGA, compresses it and recieves the compressed cube back. It is possible to use three different binning methods, median binning with SIMD instructions, mean binning with SIMD instructions or mean binning with ordinary CPU instructions. Three different functions has been written that captures frames and binns them, the only differences between them is the binning method. One gets called from the \textit{run\_cube\_capture} function, the one that is called is decided by the \textit{binning\_method} variable in the header file. The reason for having three possibilities is if further testing is needed later in the project.

The samples from the camera are in little endian order, the processing system uses big endian and the CCSDS-123 implementation needs little endian as input. That means the entire frame needs to be byte swapped to big endian before binning can be done, then the binned value has to be swapped back to little endian again. The byte swapping is done with the \textit{byteswap.h} C++ library.

To save room the bits are packed, the packing is done with help from the compiler, ARM compilers have extensions for byte packing\cite{armpacking}. Using the \textit{\#pragma pack(push[,n])} directive sets the byte alignment and using the \textit{\#pragma pack(pop)} sets the alignment back to the default value again. Bit fields is another compiler directive for C compilers, it tells the compiler that a variable only is going to use a certain amount of bits, then the compiler can pack adjacent bit fields to share bytes\cite{bitfields}.

Listing \ref{lst:bitpack} shows how bit packing of 12-bits is done in the \textit{ShareMemParams} header file. The \textit{uint12\_t} struct defines a bitfield of 12 bits, this is struct is told to be one byte aligned. The \textit{dma\_data} struct creates a array of \textit{uint12\_t} struct that gets packed together.

\begin{lstlisting}[language=C++, caption={Bit packing}, label={lst:bitpack}]
#pragma pack(push, 1)
typedef struct {
	uint16_t value:12;
} uint12_t;
#pragma pack(pop)

struct dma_data {
	uint12_t buffer[CUBE_SIZE];
	unsigned int length;
};
\end{lstlisting}

After the cube is captured, the cube is stored in the format specified in the Cube format variable, or not at all if that is specified. The cube is then transferred using the functions in the \textit{CubeDMADriver} module and the caches flushed and cleaned.  When the compressed cube is received in the processing system, it is stored.


\subsection{DMAMemoryManager}

The \textit{DMAMemoryManager} is largely described in chapter\ref{chap::kernelmodule}. The init function is always run when the module is initialized, this function initializes two channels or devices for DMA, one for receiving and one for sending. The driver holds one struct called \textit{device\_data} for each channel, shown in listing \ref{lst::driverchannel}

\begin{lstlisting}[language=C++, caption={Device data}, label={lst::driverchannel}, numbers=none]
struct device_data{
	int    major_number;
	struct class*  p_device_class;
	struct device* p_device;
	struct cdev cdev;
	struct dma_data *p_dma_data;
};

static struct device_data dma_channel[2]; //0=send 1=recieve
\end{lstlisting}

This \textit{device\_data} struct holds the major number for the devices, device pointer, class pointer and a \textit{dma\_data} pointer. The \textit{dma\_data} pointer is a pointer to the DMA send and recieve memory region, this is used when cleaning and flushing cache.


\section{CSP Service}
The CSP protocol is written in C and the design is written in C++, therefore a C interface have been written for the design. It consists of two files, one cpp file that wraps the C++ functions, this is shown in listing \ref{lst::cppwrap} and one header file that decleares the function wrappers and a void pointer that is re casted to point to C++ objects, this is shown in listing \ref{lst::hwrap}.

\begin{lstlisting}[language=C++, caption={Wrapper for C++}, label={lst::cppwrap}]
extern "C"
{
    CameraHandle_C create_camera_handle() {
      return new HSICamera();
    }

    void  initialize_camera(CameraHandle_C p, double exposureMs, int rows, int columns, int frames, double fps, cubeFormat cube) {
      return ((HSICamera *)p)->initialize(exposureMs, rows, columns, frames, fps, cube);
    }

    void run_camera(CameraHandle_C p) {
      return ((HSICamera *)p)->runCubeCapture();
    }
}
\end{lstlisting}

\begin{lstlisting}[language=C++, caption={Header for C}, label={lst::hwrap}]
typedef void * CameraHandle_C;
CameraHandle_C create_camera_handle();
void    initialize_camera(CameraHandle_C p, double exposureMs, int rows, int columns, int frames, double fps, cubeFormatC cube);
void    run_camera(CameraHandle_C p);
\end{lstlisting}

A simple CSP service for the payload node has been written and a client for the ground node that can send commands to the service. The service connect, bind and listens on a socket, then it accepts a connection if a client connects to it. When a connection is accepted and the packet received, the packet is read, the first four bytes of the packet is the command, this service only uses two commands, initialize and capture. The initialize routine creates the \textit{HSICamera} module and runs the initialization function. This function requires six arguments, as described in chapter \ref{chap::systemdesign}, the arguments is extracted from the CSP packet and the function run. The capture routine of the CSP service runs the cube capturing function.

On the client side, two functions is needed to call the service with the commands. These functions must be added as a command to the CSP terminal, this is done with the function in listing \ref{lst::addscpclient}. This tells the CSP network that when the init command is used for this node, the \textit{cli\_hsi\_init} is run, and the same goes for \textit{cli\_hsi\_capture} when the capture command is used.

\begin{lstlisting}[language=C++, caption={Adding CSP commands}, label={lst::addscpclient}]
typedef void * CameraHandle_C;
void cli_hsi_init_cmds(cli_cmd *c_hsi)
{
	cli_cmd *c_hsi_init =
		cli_add("init", "<CSP ID>",
			"Cli hsi init.", cli_hsi_init, c_hsi);

	cli_cmd *c_hsi_capture =
		cli_add("capture", " ...", " ... ", cli_hsi_capture, c_hsi);
}
\end{lstlisting}

In the \textit{cli\_hsi\_init}, the CSP packet is packaged with the command and data, this data is the initialization variables for the camera and are user inputs. The client connects to the CSP node and service. The node is identified with a ID number, this ID is set when the CSP nodes are built and compiled. When a connection is accepted the package sent to the service. The same pattern goes for the \textit{cli\_hsi\_capture} as well.

Figure \ref{fig::sequence} shows a sequence diagram of how one cube is captured with a CSP service. The client sends the init command with parameters to the service, this opens the camera  and runs \textit{initialize} function in \textit{HSICamera}, that configures the camera and allocates memory. Then a capture cube command is sent from the client, this runs the cube capturing routine in the \textit{HSICamera}, this routine capture frames from the camera and bins them, then stores them. Then the \textit{CubeDMADriver} gets initialized for data transfer. Then the cache is cleaned, a cube is transfered, compressed and transfered back again and the caches flushed. Then the cube is stored and the camera closed.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{images/sequence.png}
	\caption{Sequence diagram of cube capturing}
	\label{fig::sequence}
\end{figure}

\section{Challenges}

A few other methods for memory allocation have been tried before the chosen solution was arrived on. The function \textit{kmalloc (size\_t size, gfp\_t flags)} allocates a contiguous region of memory, that can be set as non movable, meaning that the kernel won't swap the pages out of memory. It returns a pointer to a kernel logical address of the size specified. The maximum size this function can allocate on this particular system is 4MB making it unusable for a cube.

Another function that is made for the specific purpose of allocating memory for DMA transfer is \textit{dma\_alloc\_coherent(struct device *dev, size\_t size, dma\_addr\_t *dma\_handle, gfp\_t flag)}. The arguments for this function is a device structure which contains information on which driver it is, and a dma\_handle struct that can be used for different kernel support when it comes to DMA. The size of the region and flags for telling the kernel what type of memory it is and how the operation should be executed. The return value is a pointer to kernel virtual address space. If there is room for the entire memory space in main memory, the memory space is allocated there. If there is not room for this memory space, the kernel tries to move pages out of main memory to make room. The kernel can't move pages that are marked as non movable, and if non movable pages make main memory too fragmented, the function fails.

Memory fills up with non movable pages quite quickly, the largest amount of memory allocated on the zedboard with \textit{dma\_alloc\_coherent} is 15MB. Linux has a framework for contiguous memory allocation, this is called CMA. It can reserve memory from the kernel at boot time. This without any rebuild of the kernel, arguments are simply passed to the kernel at boot time, that specifies the size of memory and which kernel modules that are allowed to use it. These modules can allocate this memory with CMA functions or when the \textit{dma\_alloc\_coherent} function is used, the function use the CMA memory automatically. The problem with this was that the kernel wouldn't give up more than 64MB to CMA, for unknown reasons. It is also possible to pass arguments to the kernel at boot time that tells it to reserve memory or to not use the whole memory, this also proved to be unsuccessful for unknown reasons.

There have been some problems with the SD cards, after a while some of them break. A Linux terminal program called fsck have been used to fix those who have been possible to fix, but not all are fixable.
\chapter{Analysis}
\label{chap:analDisc}
This chapeters discusses a few examples on how frames can be captured, and that compromises have to made to achieve higher rates for resolution and framerate. The resulting low framerate and possible reasons are discussed. Also a few additional issues in the image pipeline design is discussed. Steps that has to be done and improvements is presented and a short conclusion is given at the end.

\section{Discussion}

In the specialization project, it was concluded that mean binning with SIMD were preferable before median and mean binning with regular CPU instructions. This can also be confirmed with the results in table \ref{tab:fpsresults2}, where mean binning with SIMD instructions reaches a higher FPS than with regular CPU instructions. The table also shows that median binning with SIMD is faster than mean binning with SIMD. In addition to the faster speed, median binning gave a better S/N ratio than mean binning in the testing performed in the specialization project.

The framerates achieved for median binning is still relatively low, but it is possible to increase the framerate consideribly. With subsampling in the spectral direction of x2 the speedup achieved is up to 2-3 times, depending on the resolution. For subsambling x4, even faster rates are possible. If these results are compared to the results achieved with subsampling x4 and no binng, one can see that the results are similar. Here the driver limits the framerate for all resolutions, so this is the maximum possible rates for the chosen camera with the available driver, hardware and software.

What limits the framerate with SIMD and no subsampling and subsampling x2 is probably a combination of the CPU speed and the moving of data into and out of the SIMD core. The trade-off here is between the spectral resolution and framerate, with subsampling fewer bands are captured by the camera but a higher FPS is reached. Finding the optimal trade-off between FPS is and the spectral resolution is outside of the scope of this thesis, but the conclusion based on the performed tests is that it is not possible to achieve the maximum framerate with this set-up without sacrificing data.


When it comes to HW-trigger vs Freerun mode, it does not seem to make any difference in terms of stability or CPU usage. The easiest solution would then be to use freerun mode, since it does not require any additional hardware on the satellite, compared with HW-triggering, pins and wires from a source signal with the right clock frequency has to be set up.

Because of memory constraints it is not possible to fit the goal of 2254 frames with 1936x1216 resolution and 12-bit depth in memory. Due to this, binning is necessary. Without using binning, an acquired cube is of size $2254\times1936\times1216=5306312704$ pixel samples, and with 12-bit samples one cube would require $\frac{5306312704*12-bit}{8-bit}=7.95GB$, which is more than it is room for on the 1GB memory. But it might be possible to store smaller cubes to the SD-card while the frames are captured, then send each cube to be compressed one by one and then store each compressed cube to the SD-card. But this would use time, in the specialization project, storing of one cube with the size of 360MB takes 7-9 seconds. If this is to be done for almost 8GB of data, it is going to take a considerable amount of time. This cube would have to be first stored, then read and sent to the FPGA, then stored again, and then read again to be sent over the CSP bus. This would easily break the timing constraint of having the data ready 70 seconds after the last frame is captured.

Since it is not possible to reach the goal of capturing 2254 12-bit images at a resolution of 1216x1936 with a framerate of 32 fps with this hardware, binning and subsampling can be used. In this way, the produced cube can fulfill the requirements. For example, with subsampling x2 and binning factor of 8, a framerate of 16 fps is achieved with full resolution. A cube like this uses 497MB of memory which fits well into the constraints of the Picozed. If a framerate of 32 fps is necessary, the same subsampling and binning factor can be used with a resolution of 1024x1024, from table \ref{tab:fpsresults2} this gives a framerate of up to 36 fps, a cube like this uses 221MB of memory. Another possibility is to capture fewer frames, if subsampling x2 is used with 2254 1024x1024 frames and no binning, the framerate from table \ref{tab:fpsresults2} is 37 fps and the space the cube occupies is 1.77GB, but if only 1100 frames is captured, it would use 865MB. A potential scenario can include that the optics does not cover the whole image sensor, the dark part of the sensor are then not needed, this will be contributing factor to what the area of interest will finally be.

The CCSDS-123 implementation reads and writes continuously, meaning that the output stream does not get delayed until the whole input stream is read. In this project there have been an attempt to delay the output stream with FIFO's on the output of the CCSDS-123 block, but the FPGA resources are limited. The deepest FIFO IP available in Vivado is 32768 bits deep and utilizes 65 BRAM elements. From the results in the CCSDS-123 papers \cite{ccsdspaper1}\cite{ccsdspaper2}, the average compression ratio is 3. If for example a cube of 500MB is to be compressed, the compressed file is likely to be 166MB, then about 5066 FIFO blocks are needed. From figure \ref{fig::utilizationall} one can see that 43.5 of the 140 available BRAMs is used and that if one FIFO uses 65 BRAM elements that 5066 FIFO block abviously will exceed the FPGAs limit. Also the biggest chip available for a Picozed board has 265 BRAM blocks, that is also definitively too small. This means that there has to be room for the compressed cube in memory when CCSDS-123 streams data out.

With a average compression ratio of 3, there must be room for the compressed image that is at least one third of the size of cube. This means that if 800MB is allocated for DMA memory, at least 200MB of that must be dedicated to the compressed cube. It might also be possible to receive the compressed cube at the same location as the uncompressed cube is read from. It needs to be ensured that writing and reading operations are not performed at the same memory address, since in this way data corruption can happen, this has not been tested for.



The gigabit ethernet standard has a speed of 1000Mbps, 12-bit pixels are stored in 16-bit words in memory. From the manual\cite{ueyemanual} it says that data is sent over ethernet in 32-bit words. Efforts have been made to get more information of the communication protocol and the camera driver from IDS, but they have been reluctant to give it out. Therefore it is assumed that the 12-bit words are zero padded to 16-bit words in the camera and then packed into 32-bit words and sent over ethernet. This would mean that for full resolution, a framerate of 26-27 should be achieved, based on the theroretical speed of the ethernet protocol. This is not the case, as seen in table \ref{tab:fpsresults}.
On a PC, FPS of 24 is reached with full resolution, this is not as fast as the theoretical speed, but this rules out the possibility that it is the camera or ethernet connection that is limiting the speed.

The CPU utilization is high for the three first 16-bit values in figure \ref{fig::bps} and the throughput is considerably lower than for the rest of the resolutions. But for the rest of the resolutions the throughput averages around 49MBps, which is under half of what the theoretical speed is for gigabit ethernet(1Gbps is the same as 125MBps). This and the fact that throughput goes up as CPU utilization goes down could indicate that for the the high resolutions with high bit-depth, there are many pixels and loads of data to process as they are received and stored in memory, therefore it could be the CPU who limits the the speed in these cases. Then as there are fewer pixels to process, the speed goes up and averages around about 49MBps. The reason for the low 49MBps average must be something else than the CPU then, and out from figure \ref{fig::soc}, there is a Ethernet peripheral between the bus and the multiplexer, it is possible that this is the bottle neck.

It would probably have been a more pretty and generic solution to use the CMA framework embedded in Linux instead of making a reserved memory node and remapping the memory back. But when debugging the problem, no reason for it not work was found, that does not mean that no solution to the problem exist. Someone with more knowledge about Embedded Linux systems and petalinux might figure it out, but because of the time left of the project and and the fact that the current DMA solution actually does the job, it was decided to go for that, rather than use more time figuring out why CMA does not work.

There are more ways of swapping endianess and packing bits than the ways it have been done in this project. But since it seems like it does not have a significant effect on the execution time in this project, more options have not been tested. Since the driver uses a interface written in C and the CSP services is written in C, it might have been easier to write the \textit{HSICamera} and \textit{CubeDMADriver} modules in C as well. Then a extra C interface does not have had to be written for the CSP service.

It is somewhat worrying that some SD cards break so easily, this can be a problem in flight. It is not easy to fix or change a broken SD card in space. A solution can be to not use cheap cards, and instead use more more rugged industrial cards. It also exists SD cards that have been tested for space, but more research on this has to be done in the HYPSO project.
%Using other boards
%Use other camera

\section{Future work}
There is quite some work left before this design can be used on the finished satellite, the most important thing is to make the design more reliable and rigid, in this thesis there have been little or no focus on error handling and fault prevention. This thesis have only set up a basic design and image processing chain that can be built on to make a end result that can run on the satellite.

The actual specifications on the software design have not been decided upon yet, with regards to if or how much subsampling is acceptable, how many bands can be binned, how much freedom to change the AOI of the sensor, how many frames to capture in flight and camera sensor settings. As of now the user is free to change the exposure time of the sensor the size of the AOI, number of frames to capture, the FPS and what cube format the binned cube is stored in. This may have to be changed in the design in the future when the specifications have been agreed upon, so there have been an attempt to make the design generic for now and easy to change and build on later.

Other things that has to be in place is the insertion of the kernel module, now the module is inserted with the command line after the OS has booted. It is much more convenient to insert it at boot time, this should be a easy thing to build in petalinux. There has also been discovered a issue at the end of this thesis, with the compressed cube. Sometimes there are a few trailing zeros at the end of the compressed file, the numbers of zeros have been observed to be either none or four, but not enough observations have been made to determine this. One cause for this problem could be at the output of the CCSDS-123 block in the FPGA, it may be possible that when the output bitstream of CCSDS-123 block is larger than the last output word from the algorithm, that it gets zero padded at the end. For example if the output bitstream is 64(The Bus width in table \ref{tab::ccsds}) and the last output word from the algorithm is 32, then the 32 last bits is set to zero, making there be four trailing zeros at the end of the compressed file. This is just a theory, this error has to be detected and fixed.

As discussed earlier that writing to the same address as the cube DMA is starting to read from might be possible, this would make it possible to use the whole DMA memory for a uncompressed cube, before it is compressed. This has to be tested and proven before it can be done and would make a huge improvement to the amount of data in the cube.

One feature that could be nice to have is the ability to switch the endianess of the input to the CCSDS-123 module in the FPGA, this would make it possible to send both big endian and little endian numbers to the CCSDS-123 accellerator in the FPGA. It is not a extremely necessary feature to have since it did not take a lot of resources to switch endianess in the processing system, but for future missions where the CCSDS-123 algorithm is used things might be different and it could be inconvenient to have to swapp endianess in the processing system. The endianess switching could be a separate module in the FPGA or as an modification of the existing CCSDS-123 module.

It might be worth doing further investigations of why the speed is so degraded when transferring frames from the camera to the processing system. One thing that can be done is to implement an Gigabit Ethernet core in the FPGA and connect it to the EMIO pins between the FPGA ans the multiplexer, see figure \ref{fig::soc} to see where the EMIO pins are located. Then use the AXI interface to communicate with this module, the GigE preipheral is then bypassed, revealing if this is the bottle neck for the low FPS when capturing frames.

There should also be a timestamp for each frame that is captured, this could be put together to work in several different ways. It can happen on a separate microcontroller, some other hardware component or the SOC capturing the frames. If the timestamping is done on this chip, the driver API or the flash pin on the camera can be used as the signal for a image being captured. The timing diagram in figure \ref{fig:triggerHW} and \ref{fig:freerun} shows how image capturing, API events and pin signals are synchronized in relation to each other. Since the timing of transfer and pre-processing can be variable, it is probably most stable to not use the API event and instead use the flash pin, the trigger pin can also be used if HW-triggering mode is used and not freerun mode. The signal from the camera can be connected to a interrupt pin and then a small program that logs the time of the signal can be run, this could in turn slow down the image acquisition, therefore it might be best to do this on a separate piece of hardware, but this have to be tested for.

\section{Conclusion}
To conclude with. It is not possible to get the desired rates for FPS and resolution with this hardware and camera, the memory is simply too small, the processor too slow and the transfer speed between camera and processing system is too limited. But it is however possible to reduce the framerate and resolution and reach some compromise of what is acceptable for the data. A merging network in SIMD have been shown to make median binning possible and even faster than mean binning is in SIMD.

The operating system supports DMA access, the DMA core and CCSDS-123 core in programmable logic can access DMA memory and compress the data there. There is more work to be done on the design before it is space ready and there is room for improving and testing some things. The possibility of writing to the same address as is written from when compressing the cube should be investigated, also the problem with trailing zeros on the compressed cube must be solved.

Now that it is somewhat clear what capabilities this camera and hardware have and what rates can be expected. Specifications for the final version of the image acquisition pipeline should be made, in relation to what features and options on for example resolution, framerate and binning types should be available, and how generic things should be. Making it easier to continue the work with some clear goals of what the design is actually supposed to do.



\bibliographystyle{plain}
\bibliography{biblio.bib}

\chapter{Appendices}
\section{Appendix 1: Tutorial}
\includepdf[pages={1-13}]{tutorial.pdf}
\end{document}
